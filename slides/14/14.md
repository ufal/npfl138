title: NPFL138, Lecture 14
class: title, langtech, cc-by-sa

# Speech Synthesis

## Milan Straka

### May 20, 2025

---
section: Mel Spectrograms
class: section
# Mel Spectrograms

---
# Speech Generation Pipeline

![w=65%,h=center](speech_synthesis_pipeline.png)

---
# Speech Generation Pipeline

![w=35%,f=right](speech_synthesis_pipeline.png)

Traditionally, the input text was represented by **phonemes**.
![w=50%](phonemic_chart.png)

~~~
However, nowadays the input is most commonly represented using **characters**
(some systems still allow specifying some input words using phonemes to allow
more control over the output).

---
# Time vs Frequency Domain

![w=60%,h=center](time_vs_frequency.png)

A sound can be represented in the **time domain**, as a sequence of
amplitudes, i.e., how the signal changes over time.

~~~
Alternatively, it can be represented in a **frequency domain**, where
a signal is represented by a _spectrum_: the magnitudes and phases of sinusoids
with a collection of frequencies.

---
# Time vs Frequency Domain

![w=35%,f=right](time_vs_frequency.png)

To convert a signal between time and frequency domains, Fourier transform can be
used. In case of discrete-time signal, **discrete Fourier transform (DFT)** converts
~~~
- $N$ complex-valued equally-spaced samples in the time domain to
~~~

- $N$ complex-valued coefficients (magnitude and phase) of sinusoids with
  frequencies 1 Hz, 2 Hz, 3 Hz, â€¦, $N$ Hz.

~~~
**Fast Fourier transform (FFT)** is an algorithm that can perform discrete
Fourier transform in in $ğ“(N \log N)$ time for $N=2^k$ a power of two.

~~~
When the input signal consists of only real values, the upper half of the
frequency spectrum is determined by the lower half ($x_{N-i} = \overline{x_i}$,
where $\overline x$ denotes complex conjugate); therefore, a discrete signal
of $N$ samples can be represented by sinusoids with frequencies of 1, 2, 3, â€¦,
$\frac{N}{2}$.

---
# Spectrogram

![w=41.5%,f=right](stft_output.png)

We can represent an arbitrary-length signal by a **spectrogram**, a series of
fixed-length spectral densities (i.e., for every sinusoid, we compute some
real-valued density, usually _power density_ $x^2$).

~~~
The spectrogram is computed using **short-time Fourier transform (STFT)**, where
- DFT is performed on windows of fixed size,
~~~
- neighboring windows are shifted by a fixed hop length.

~~~
For every window, the signal is composed of sinusoids with frequencies up
to half of the window length.

~~~
In speech synthesis, assuming a sampling frequency of 22.05kHz or 24kHz,
commonly used values are 1024 for window length (denoted
commonly as _n_fft_; ~45ms) and hop length of 256 (circa 11ms).

---
# Spectrogram

![w=41.5%,f=right](stft_output.png)

To avoid discontinuities at the borders of the windows, _windowing_ is used.

![w=57%](windowing.png)

~~~
![w=24.5%,f=right](hann_window.png)

Many possible window function exist; we will use the **Hann** window,
which is as suitably scaled cosine function.

---
# Mel Spectrogram

Linearly increasing frequencies are not perceived to be equal in distance from
one another by humans.

~~~
Therefore, the **mel scale** (from the word melody) was proposed, so that the
pitches were judged by listeners to be in equal distance.

~~~
![w=62%,f=right](mel_hz_plot.svgz)

There are in fact several mel scales, the most commonly used is
$$m = 2595 \log_{10} \Big(1 + \frac{f}{700}\Big).$$

---
# Mel Spectrogram

To convert spectrogram to mel spectrogram, a collection of triangular filters
equally-distanced in the mel scale is used.

![w=95%,h=center](mel_filterbank.svgz)

~~~
Finally, we usually take the logarithm of the power spectrum, given that
humans percieve loudness on logarithmic scale (i.e., dBs are also a logarithmic scale).

---
# Mel Spectrogram

![w=55.5%,h=center](mel_spectrogram.svgz)

---
# Speech Generation Pipeline Revisited

![w=55%,h=center](speech_synthesis_pipeline.png)

In the context of speech synthesis, a **vocoder** (combination of words
voice and encoder) is the second part of the pipeline, converting
a (usually mel) spectrogram to a waveform.

---
section: WaveNet
class: section
# WaveNet

---
# WaveNet

WaveNet, proposed in 2016, was one of the first neural architectures that
produced high quality human speech.

~~~
Not many details were published in the original paper. From today's perspective,
WaveNet is a vocoder capable of converting mel spectrograms to waveforms.

~~~
In the following, we start the description without the mel spectrogram
conditioning, but we add it later.

---
# WaveNet

Our goal is to model speech, using a convolutional auto-regressive model
$$P(â†’x) = âˆ_t P(x_t | x_{t-1}, â€¦, x_1).$$

~~~
![w=80%,h=center](wavenet_causal_convolutions.svgz)

---
# WaveNet

However, to achieve larger receptive field, we utilize **dilated** (or **atrous**) convolutions:

![w=100%,v=middle](wavenet_dilated_convolutions.svgz)

---
class: tablewide
style: table {line-height: 1}
# Dilated Versus Regular Versus Strided Convolutions

| Regular Convolution | Strided Convolution |
|:--:|:--:|
| ![w=33%,h=center](../04/conv_valid_padding_no_strides.gif) | ![w=37%,h=center](../04/conv_same_padding_strides.gif) |
| **Dilated Convolution** | **Transposed Strided Convolution** |
| ![w=39.5%,h=center](conv_dilation.gif) | ![w=33%,h=center](../05/conv_same_padding_strides_transposed.gif) |

---
# WaveNet â€“ Output Distribution

## Output Distribution

WaveNet generates audio with 16kHz frequency and 16-bit samples.

~~~
However, classification into $65\,536$ classes would not be efficient. Instead,
WaveNet adopts the $Î¼$-law transformation, which passes the input samples in
$[-1, 1]$ range through the $Î¼$-law encoding
$$\sign(x)\frac{\log(1 + 255|x|)}{\log(1 + 255)},$$
and the resulting $[-1, 1]$ range is linearly quantized into 256 buckets.

~~~
The model therefore predicts each samples using classification into 256 classes,
and then uses the inverse of the above transformation on the model predictions.

<audio controls style="width: 49%"><source src="https://upload.wikimedia.org/wikipedia/commons/f/f4/Larynx-HiFi-GAN_speech_sample.wav"></audio>
<audio controls style="width: 49%"><source src="https://upload.wikimedia.org/wikipedia/commons/d/da/Mu-law_audio_demo.flac"></audio>

---
# WaveNet â€“ Architecture

![w=68%,h=center](wavenet_block.svgz)

~~~
The outputs of the dilated convolutions are passed through the _gated activation
unit_:
$$â†’z = \tanh(â‡‰W_f * â†’x) âŠ™ Ïƒ(â‡‰W_g * â†’x).$$

---
# WaveNet

## Global Conditioning
Global conditioning is performed by a single latent representation $â†’h$,
changing the gated activation function to
$$â†’z = \tanh(â‡‰W_f * â†’x + â‡‰V_fâ†’h) âŠ™ Ïƒ(â‡‰W_g * â†’x + â‡‰V_gâ†’h).$$

~~~
## Local Conditioning
For local conditioning, we are given a time series $â†’h$, possibly with a lower
sampling frequency (for example the mel spectrogram). We first use transposed
convolutions $â†’y = f(â†’h)$ to match resolution and then compute analogously to
global conditioning
$$â†’z = \tanh(â‡‰W_f * â†’x + â‡‰V_f * â†’y) âŠ™ Ïƒ(â‡‰W_g * â†’x + â‡‰V_g * â†’y).$$

The authors mention that using repetition instead of transposed convolution
worked slightly worse.

---
# WaveNet

The original paper did not mention hyperparameters, but later it was revealed
that:
- 30 layers were used

~~~
  - grouped into 3 dilation stacks with 10 layers each
~~~
  - in a dilation stack, dilation rate increases by a factor of 2, starting
    with rate 1 and reaching maximum dilation of 512
~~~
- kernel size of a dilated convolution is 2 (and increased to 3 in Parallel
  WaveNet)
~~~
- residual connection has dimension 512
~~~
- gating layer uses 256+256 hidden units
~~~
- the $1Ã—1$ convolutions in the output step produce 256 filters
~~~
- trained for $1\,000\,000$ steps using Adam with a fixed learning rate of 2e-4

---
# WaveNet

![w=85%,h=center](wavenet_results.svgz)

---
section: GLUs
class: section
# Gated Activations in Transformers

---
class: tablewide
style: table { line-height: 1.0 }
# Gated Activations in Transformers

Similar gated activations seem to work the best in Transformers, in the FFN
module.

~~~
| Activation Name | Formula | $\operatorname{FFN}(x; â‡‰W_1, â‡‰W_2)$ |
|:----------------|:-------:|:---:|
| ReLU            | $\max(0, x)$ | $\max(0, â†’xâ‡‰W_1)â‡‰W_2$
~~~
| GELU            | $x Î¦(x)$ | $\operatorname{GELU}(â†’xâ‡‰W_1)â‡‰W_2$
~~~
| Swish           | $x Ïƒ(x)$ | $\operatorname{Swish}(â†’xâ‡‰W_1)â‡‰W_2$
~~~

There are several variants of the new gated activations:

| Activation Name | Formula | $\operatorname{FFN}(x; â‡‰W, â‡‰V, â‡‰W_2)$ |
|:----------------|:-------:|:---:|
| GLU (Gated Linear Unit) | $Ïƒ(â†’xâ‡‰W + â†’b)âŠ™(â†’xâ‡‰V + â†’c)$ | $(Ïƒ(â†’xâ‡‰W)âŠ™â†’xâ‡‰V)â‡‰W_2$
~~~
| ReGLU | $\max(0, â†’xâ‡‰W + â†’b)âŠ™(â†’xâ‡‰V + â†’c)$ | $(\max(0, â†’xâ‡‰W)âŠ™â†’xâ‡‰V)â‡‰W_2$
| GEGLU | $\operatorname{GELU}(â†’xâ‡‰W + â†’b)âŠ™(â†’xâ‡‰V + â†’c)$ | $(\operatorname{GELU}(â†’xâ‡‰W)âŠ™â†’xâ‡‰V)â‡‰W_2$
| SwiGLU | $\operatorname{Swish}(â†’xâ‡‰W + â†’b)âŠ™(â†’xâ‡‰V + â†’c)$ | $(\operatorname{Swish}(â†’xâ‡‰W)âŠ™â†’xâ‡‰V)â‡‰W_2$

---
# Gated Activations in Transformers

![w=73.8%](gated_transformers_glue.svgz)![w=26.2%](gated_transformers_squad.svgz)

![w=100%](gated_transformers.svgz)

---
section: ParallelWaveNet
class: section
# Parallel WaveNet

---
style: .katex-display { margin: .8em 0 }
# Parallel WaveNet

Parallel WaveNet is an improvement of the original WaveNet by the same authors.

~~~
First, the output distribution was changed from 256 $Î¼$-law values to a Mixture of
Logistic (suggested in another paper â€“ PixelCNN++, but reused in other architectures since):
$$x âˆ¼ âˆ‘_i Ï€_i \operatorname{Logistic}(Î¼_i, s_i).$$

~~~
![w=28%,f=right](logistic_pdf.svgz)

The logistic distribution is a distribution with a $Ïƒ$ as cumulative density function
(where the mean and scale is parametrized by $Î¼$ and $s$).
~~~
Therefore, we can write
$$P(x | â†’Ï€, â†’Î¼, â†’s) = âˆ‘_i Ï€_i \bigg[Ïƒ\Big(\frac{x + 0.5 - Î¼_i}{s_i}\Big) - Ïƒ\Big(\frac{x - 0.5 - Î¼_i}{s_i}\Big)\bigg],$$
where we replace $-0.5$ and $0.5$ in the edge cases by $-âˆ$ and $âˆ$.

~~~
In Parallel WaveNet teacher, 10 mixture components are used.

---
style: .katex-display { margin: .8em 0 }
# Parallel WaveNet

Auto-regressive (sequential) inference is extremely slow in WaveNet.

~~~
Instead, we model $P(x_t)$ as $P(x_t | â†’z_{<t})
= \operatorname{Logistic}\big(x_t; Î¼^1(â†’z_{< t}), s^1(â†’z_{< t})\big)$
for a _random_ $â†’z$ drawn from a logistic distribution
$\operatorname{Logistic}(â†’0, â†’1)$. Therefore, using the reparametrization
trick,
$$x^1_t = Î¼^1(â†’z_{< t}) + z_t â‹… s^1(â†’z_{< t}).$$

~~~
Usually, one iteration of the algorithm does not produce good enough results
â€“ consequently, 4Â iterations were used by the authors. In further iterations,
$$x^i_t = Î¼^i(â†’x^{i-1}_{< t}) + x^{i-1}_t â‹… s^i(â†’x^{i-1}_{< t}).$$

~~~
After $N$ iterations, $P(â†’x^N_t | â†’z_{<t})$ is a logistic distribution
with location $â†’Î¼^\textrm{tot}$ and scale $â†’s^\textrm{tot}$:
$$Î¼^\textrm{tot}_t = âˆ‘_{i=1}^N Î¼^i(â†’x^{i-1}_{< t}) â‹… \Big(âˆ\nolimits_{j>i}^N s^j(â†’x^{j-1}_{< t})\Big) \textrm{~~and~~}
  s^\textrm{tot}_t = âˆ_{i=1}^N s^i(â†’x^{i-1}_{< t}),$$
where we have denoted $â†’z$ as $â†’x^0$ for convenience.

---
style: .katex-display { margin: .8em 0 }
# Parallel WaveNet

The consequences of changing the model from $x_t = f(x_{t-1}, â€¦, x_1)$ to
$$\begin{aligned}
x^1_t &= Î¼^1(â†’z_{< t}) + z_t â‹… s^1(â†’z_{< t}) \\
x^i_t &= Î¼^i(â†’x^{i-1}_{< t}) + x^{i-1}_t â‹… s^i(â†’x^{i-1}_{< t}) \\
\end{aligned}$$

are:
~~~
- During inference, the prediction can be computed in parallel, because
  $x^i_t$ depends only on $â†’x^{i-1}_{< t}$, not on $x^i_{< t}$.
~~~
- However, we cannot perform training in parallel.
~~~
  If we try maximizing the log-likelihood of an input sequence $â†’x^1$,
  we need to find out which $â†’z$ sequence generates it.
~~~
  - The $z_1$ can be computed using $x^1_1$.
~~~
  - However, $z_2$ depends not only on $x^1_1$ and $x^1_2$, but also on $z_1$;
    generally, $z_t$ depends on $â†’x^1$ and also on all $â†’z_{< t}$, and can be
    computed only sequentially.

~~~
Therefore, WaveNet can perform parallel training and sequential inference, while
the proposed model can perform parallel inference but sequential training.

---
# Probability Density Distillation

The authors propose to train the network by a **probability density distillation** using
a teacher WaveNet (producing a mixture of logistic with 10 components) with KL-divergence as a loss.

![w=75%,h=center](parallel_wavenet_distillation.svgz)

---
style: .katex-display { margin: .8em 0 }
# Probability Density Distillation

Therefore, instead of computing $â†’z$ from some gold $â†’x_g$, we
~~~
- sample a random $â†’z$;
~~~
- generate the output $â†’x$;
~~~
- use the teacher WaveNet model to estimate the log-likelihood
  of $â†’x$;
~~~
- update the student to match the log-likelihood of the teacher.

~~~
Denoting the teacher distribution as $P_T$ and the student distribution
as $P_S$, the loss is
$$D_\textrm{KL}(P_S || P_T) = H(P_S, P_T) - H(P_S).$$

~~~
Therefore, we do not only minimize cross-entropy, but we also try to keep the
entropy of the student as high as possible â€“ it is indeed crucial not to match
just the mode of the teacher.
~~~
- Consider a teacher generating white noise, where every sample comes from
  $ğ“(0, 1)$ â€“ in this case, the cross-entropy loss of a constant $â†’0$, complete
  silence, would be maximal.

~~~
In a sense, probability density distillation is similar to GANs. However,
the teacher is kept fixed, and the student does not attempt to fool it
but to match its distribution instead.

---
class: dbend
# Probability Density Distillation Details

Because the entropy of a logistic distribution $\operatorname{Logistic}(Î¼, s)$
is $\log s + 2$, the entropy term $H(P_S)$ can be rewritten as follows:
$$\begin{aligned}
H(P_S) &= ğ”¼_{zâˆ¼\operatorname{Logistic}(0, 1)}\left[\sum_{t=1}^T - \log p_S(x_t|â†’z_{<t})\right] \\
       &= ğ”¼_{zâˆ¼\operatorname{Logistic}(0, 1)}\left[\sum_{t=1}^T \log s(â†’z_{\lt t}, â†’Î¸)\right] + 2T.
\end{aligned}$$
Therefore, this term can be computed without having to generate $â†’x$.

---
class: dbend
# Probability Density Distillation Details

However, the cross-entropy term $H(P_S, P_T)$ requires sampling from $P_S$ to estimate:

~~~
$\displaystyle \kern6em{}\mathllap{H(P_S, P_T)} = âˆ«_{â†’x} -P_S(â†’x) \log P_T(â†’x)$

~~~
$\displaystyle \kern6em{} = âˆ‘_{t=1}^T âˆ«_{â†’x} -P_S(â†’x) \log P_T(x_t|â†’x_{<t})$

~~~
$\displaystyle \kern6em{} = âˆ‘_{t=1}^T âˆ«_{â†’x} -\textcolor{blue}{P_S(â†’x_{<t})}\textcolor{green}{P_S(x_t|â†’x_{<t})}\textcolor{red}{P_S(â†’x_{>t}|â†’x_{\leq t})} \log P_T(x_t|â†’x_{<t})$

~~~
$\displaystyle \kern6em{} = âˆ‘_{t=1}^T ğ”¼_{\textcolor{blue}{P_S(â†’x_{<t})}} \bigg[âˆ«_{x_t} -\textcolor{green}{P_S(x_t|â†’x_{<t})} \log P_T(x_t|â†’x_{<t}) âˆ«_{â†’x_{>t}} \textcolor{red}{P_S(â†’x_{>t}|â†’x_{\leq t})}\bigg]$

~~~ ~~
$\displaystyle \kern6em{} = âˆ‘_{t=1}^T ğ”¼_{\textcolor{blue}{P_S(â†’x_{<t})}} \bigg[âˆ«_{x_t} -\textcolor{green}{P_S(x_t|â†’x_{<t})} \log P_T(x_t|â†’x_{<t}) \underbrace{\,âˆ«_{â†’x_{>t}} \textcolor{red}{P_S(â†’x_{>t}|â†’x_{\leq t})}}_1\bigg]$

~~~
$\displaystyle \kern6em{} = âˆ‘_{t=1}^T ğ”¼_{P_S(â†’x_{<t})} H\Big(P_S(x_t|â†’x_{<t}), P_T(x_t|â†’x_{<t})\Big).$

---
# Probability Density Distillation Details

$$H(P_S, P_T) = âˆ‘_{t=1}^T ğ”¼_{P_S(â†’x_{<t})} H\Big(P_S(x_t|â†’x_{<t}), P_T(x_t|â†’x_{<t})\Big)$$

We can therefore estimate $H(P_S, P_T)$ by:
- drawing a single sample $â†’x$ from the student $P_S$ _[a Logistic($â†’Î¼^\textrm{tot}, â†’s^\textrm{tot}$)]_,
~~~
- compute all $P_T(x_t | â†’x_{<t})$ from the teacher in parallel _[mixture of
  logistic distributions]_,
~~~
- and finally evaluate $H(P_S(x_t|â†’x_{<t}), P_T(x_t|â†’x_{<t}))$ by sampling
  multiple different $x_t$ from the $P_S(x_t|â†’x_{<t})$.

~~~
The authors state that this unbiased estimator has a much lower variance than
naively evaluating a single sequence sample under the teacher using the original formulation.

---
# Parallel WaveNet

The Parallel WaveNet model consists of 4 iterations with 10, 10, 10, 30 layers,
respectively. The dimension of the residuals and the gating units is 64
(compared to 512 in WaveNet).

~~~
The Parallel WaveNet generates over 500k samples per
second, compared to ~170 samples per second of a regular WaveNet â€“ more than
a 1000 times speedup.

![w=75%,h=center](parallel_wavenet_results.svgz)

~~~
For comparison, using a single iteration with 30 layers achieve MOS of 4.21.

---
# Parallel WaveNet

The Parallel WaveNet can be trained to generate speech of multiple speakers
(using the global conditioning). Because such a model needs larger capacity,
it used 30 layers in every iteration (instead of 10, 10, 10, 30).

~~~
![w=100%](parallel_wavenet_multiple_speakers.svgz)

---
class: dbend
style: .katex-display { margin: .25em 0 }
# Parallel WaveNet â€“ Additional Losses

To generate high-quality audio, the probability density distillation is not
entirely sufficient. The authors therefore introduce additional losses:
~~~
- **power loss**: ensures the power in different frequency bands is on average
  similar between the generated speech and human speech. For a conditioned
  training data $(â†’x, â†’c)$ and WaveNet student $g$, the loss is
  $$\big\|\operatorname{STFT}(g(â†’z, â†’c)) - \operatorname{STFT}(â†’x)\big\|^2.$$
~~~
- **perceptual loss**: apart from the power in frequency bands, we can use
  a pre-trained classifier to extract features from generated and human speech
  and add a loss measuring their difference. The authors propose the loss as
  squared Frobenius norm of differences between Gram matrices (uncentered
  covariance matrices) of features of a WaveNet-like classifier predicting
  phones from raw audio.
~~~
- **contrastive loss**: to make the model respect the conditioning instead of
  generating outputs with high likelihood independent on the conditioning,
  the authors propose a contrastive distillation loss ($Î³=0.3$ is used in the
  paper):
  $$D_\textrm{KL}\big(P_S(â†’c_1) || P_T(â†’c_1)\big) - Î³ D_\textrm{KL}\big(P_S(â†’c_1) || P_T(â†’c_2)\big).$$

---
class: dbend
# Parallel WaveNet â€“ Additional Losses

![w=100%](parallel_wavenet_losses.svgz)

---
section: Tacotron 2
class: section
# Tacotron 2

---
# Tacotron 2

Tacotron 2 model presents end-to-end speech synthesis directly from text.
~~~
It consists of two components trained separately:
- a seq2seq model processing input characters and generating mel spectrograms;
- a Parallel WaveNet generating the speech from Mel spectrograms.

![w=48%,h=center](tacotron.svgz)

---
style: .katex-display { margin: .6em 0 }
# Tacotron 2

![w=40%,f=right](tacotron.svgz)

The Mel spectrograms used in Tacotron 2 are fairly standard:

~~~
- The authors propose a frame size of 50ms, 12.5ms frame hop, and a Hann window.

~~~
- STFT magnitudes are transformed into 80-channel Mel scale spanning 175Hz
  to 7.6kHz, followed by a log dynamic range compression (clipping input values
  to at least 0.01).

~~~
## Architecture

The characters are represented using 512-dimensional embeddings, processed
by 5 Conv+BN+ReLU with kernel size 5 and 512 channels, following
by a single bi-directional LSTM with 512 units.

---
style: .katex-display { margin: .5em 0 }
# Tacotron 2

![w=40%,f=right](tacotron.svgz)

To make sequential processing of input characters easier, Tacotron 2 utilizes
_location-sensitive attention_, which is an extension of the additive attention.
While the additive (Bahdanau) attention computes
$$\begin{gathered}
  â†’Î±_i = \mathrm{Attend}(â†’s_{i-1}, â†’h), \\
  Î±_{ij} = \softmax\big(â†’v^\top \tanh(â‡‰Vâ†’h_j + â‡‰Wâ†’s_{i-1} + â†’b)\big),
\end{gathered}$$

~~~
the location-sensitive attention also inputs the previous cummulative attention
weights $â†’Î±Ì‚_i$ into the current attention computation:
$$â†’Î±_i = \mathrm{Attend}(â†’s_{i-1}, â†’h, â†’Î±Ì‚_{i-1}).$$

~~~
In detail, the previous attention weights are processed by a 1-D convolution with kernel $â‡‰F$:
$$Î±_{ij} = \softmax\big(â†’v^\top \tanh(â‡‰Vâ†’h_j + â‡‰Wâ†’s_{i-1} + (â‡‰F * â†’Î±Ì‚_{i-1})_j + â†’b)\big),$$
using 32-dimensional 1D convolution with kernel size 31,

---
# Tacotron 2

![w=40%,f=right](tacotron.svgz)

Usually, the current state in the attention $â†’s_{i-1}$ is represented using the decoder
cell state. However, in this model, it is beneficial for the attention to has
access to the generated spectrogram.

~~~
Therefore, a separate _attention RNN_ is used, which has to goal to represent
the â€œmel spectrogram generated so farâ€: its inputs are the decoder input
(the result of the pre-net) concatenated with the attention context vector
$â†’Î±_i^T â†’h$, and the output of this attention RNN is used as $â†’s_{i-1}$ in the
attention computation.

---
# Tacotron 2

![w=40%,f=right](tacotron.svgz)

The decoder predicts the spectrogram one frame at a time.
~~~
The predictions from the previous step are first passed through a _pre-net_
composed of 2 fully-connected ReLU layers with 256 units and concatenated with
attention context vector.

~~~
The decoder consists of 2 1024-dimensional LSTM cells and its output is linearly
projected to the predicted frame.

~~~
The stop-token is predicted from a concatenation of decoder output and attention
context vector, followed by a sigmoid activation.

~~~
The predicted spectrogram frame is post-processed by a _post-net_ composed
of 5 convolutional layers followed by batch normalization and tanh activation
(on all but the last layers), with 512 channels and kernel size 5.

~~~
The target objective is the MSE error of the spectrogram and the
decoder output both before and after the post-net.

---
# Tacotron 2

![w=65%,h=center,v=middle](tacotron_results.svgz)

---
class: middle
# Tacotron 2

![w=100%](tacotron_comparison.png)

You can listen to samples at https://google.github.io/tacotron/publications/tacotron2/

---
# Tacotron 2

![w=49%](tacotron_spectrograms.svgz)
~~~
![w=49%](tacotron_vocoders.svgz)
~~~

![w=70%,h=center](tacotron_wavenet_sizes.svgz)

---
section: FastSpeech
class: section
# FastSpeech

---
# FastSpeech

![w=64%,h=center](fast_speech_architecture.svgz)

FastSpeech performs non-autoregressive decoding of mel spectrogram from the
input text.

~~~
A hard characterâ€“spectrogram frame alignment is computed using some existing
system, and the representation of input character is then repeated according
to the alignment.

---
# FastSpeech

![w=50%,h=center](fast_speech_results.svgz)

~~~
![w=80%,h=center](fast_speech_repeats.svgz)

~~~
Later improved in FastPitch and FastSpeech 2 systems.

---
section: TTSAlign
class: section
# One TTS Alignment To Rule Them All

---
# One TTS Alignment To Rule Them All

![w=95%,h=center](tts_align_architecture.svgz)

---
style: .katex-display { margin: .8em 0 }
# One TTS Alignment To Rule Them All

![w=50%,f=right](tts_align_architecture.svgz)

Let $s_i$ denotes the $i$-th input text character, and
$â†’x_t$ be the $t$-th mel spectrogram frame.

~~~
Assuming we have access to some alignment
$$P(s_i | â†’x_t; â†’Î¸).$$

~~~
We can train the alignment to maximize the probability of all monotonic
alignments between the mel spectrogram and the input text:
$$ğ“›(â†’Î¸; â†’X, â†’s) â‰ âˆ‘_{\substack{â†’c\mathrm{~any~valid}\\\mathrm{monotonic~alignment}}} âˆ_{t=1}^T P(c_t | â†’x_t; â†’Î¸).$$

~~~
Such a loss can be computed by exploiting the CTC loss (given that efficient
implementations for it already exist).

---
# One TTS Alignment To Rule Them All

![w=35%,f=right](tts_align_convergence.svgz)

~~~
![w=63%](tts_align_alignment.svgz)

~~~
![w=85%,h=center,mw=63%](tts_align_results.svgz)

---
# Alignment Prior

For faster alignment convergence (which makes the full model training faster),
during training we might use a static 2D prior that is wider near the center and
narrower near the corners.

~~~
The complete alignment is then the normalized product of the computed alignment
and the alignment prior.

~~~
![w=100%](beta_binomial_prior.svgz)


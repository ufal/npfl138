title: NPFL138, Lecture 11
class: title, langtech, cc-by-sa

# Deep Reinforcement Learning, VAE

## Milan Straka

### Apr 29, 2025

---
section: RL
class: section
# Reinforcement Learning

---
# Reinforcement Learning

_Develop goal-seeking agent trained using reward signal._

~~~
**Reinforcement learning** is a machine learning paradigm, different from
_supervised_ and _unsupervised learning_.

~~~
![w=95%,mw=60%,h=right,f=right](robots.png)

The essence of reinforcement learning is to learn from _interactions_ with the
environment to maximize a numeric _reward_ signal.
~~~

The learner is not told which actions to take, and the actions may affect not
just the immediate reward, but also all following rewards.

---
# Reinforcement Learning Successes

![w=23%,f=right](atari_games.png)

- Human-level video game playing (_DQN_) ‚Äì 2013 (2015 Nature), Mnih. et al, Deepmind.

~~~
  - After 7 years of development, the _Agent57_ beats humans on all 57
    Atari¬†2600 games, achieving a mean score of 4766% compared to human players.

~~~

- _AlphaGo_ beat 9-dan professional player Lee Sedol in Go in Mar 2016.
~~~
  - After two years of development, _AlphaZero_ achieved best performance
    in Go, chess, shogi, being trained using self-play only.
  ![w=38%,h=center](a0_results.svgz)

~~~
- Impressive performance in Dota2, Capture the flag FPS, StarCraft II, ‚Ä¶

---
style: ul { margin-bottom: 0 }
# Reinforcement Learning Successes

- Neural Architecture Search ‚Äì since 2017

~~~
  - automatically designing CNN image recognition networks
    surpassing state-of-the-art performance (_NasNet_, _EfficientNet_, _EfficientNetV2_, ‚Ä¶)
  - also used for other architectures, activation functions, optimizers, ‚Ä¶

  ![w=30%,h=center](nasnet_blocks.svgz)
~~~
![w=48%,mw=26%,h=right,f=right](data_center.jpg)

- Controlling cooling in Google datacenters directly by AI (2018)
  - reaching 30% cost reduction
~~~
- Improving efficiency of VP9 codec (2022; 4% in bandwidth with no loss in
  quality)

---
style: ul { margin-bottom: 0 }
# Reinforcement Learning Successes

![w=65%,mw=30%,h=right,f=right](tpu_trillium.jpg)

- Designing the layout of TPU chips (AlphaChip; since 2021, opensourced)

~~~
- Discovering faster algorithms for matrix multiplication (AlphaTensor, Oct 2022),
  sorting (AlphaDev, June 2023)
~~~
- Searching for solutions of mathematical problems (FunSearch, Dec¬†2023)
~~~
- Generally, RL can be used to Optimize nondifferentiable losses
~~~
  - Improving translation quality in 2016
~~~
  ![w=35%,f=right](rlhf_overview.svgz)

  - Reinforcement learning from human feedback (_RLHF_) is used
    to train chatbots (ChatGPT, ‚Ä¶)
~~~
  - Improving reasoning of LLMs (DeepSeek R1)
~~~
  - Proving math theorems (AlphaGeometry 2)

---
section: Bandits
class: section
# Multi-armed Bandits

---
# Multi-armed Bandits

![w=50%](one-armed-bandit.jpg)
~~~
![w=40%](multi-armed-bandit.png)

---
class: middle
# Multi-armed Bandits

![w=70%,h=center,v=middle](k-armed_bandits.svgz)

---
# Multi-armed Bandits

We start by selecting an action $A_1$ (the index of the arm to use), and we
obtain a reward $R_1$. We then repeat the process by selecting an action $A_2$,
obtaining $R_2$, selecting $A_3$, ‚Ä¶, with the indices denoting the time step
when the actions and rewards occurred.

~~~
Let $q_*(a)$ be the real **value** of an action $a$:
$$q_*(a) = ùîº[R_t | A_t = a].$$

~~~
Assuming our goal is to maximize the sum of rewards $\sum_i R_i$, the optimal
strategy is to repeatedly perform the action with the largest value $q_*(a)$.

---
# Multi-armed Bandits

However, we do not know the real action values $q_*(a) = ùîº[R_t | A_t = a]$.

~~~
Therefore, we will try to estimate them, denoting $Q_t(a)$ our estimated value
of action $a$ at time $t$ (before taking the trial $t$).

~~~
A natural way to estimate $Q_t(a)$ is to average the observed rewards:
$$Q_t(a) ‚âù \frac{\textrm{sum of rewards when action }a\textrm{ is taken}}{\textrm{number of times action }a\textrm{ was taken}}.$$

~~~
Utilizing our estimates $Q_t(a)$, we define the **greedy action** $A_t$ as
$$A_t ‚âù \argmax_a Q_t(a).$$

~~~
When our estimates are accurate enough, the optimal strategy is to repeatedly
perform the greedy action.

---
style: .katex-display { margin: .5em 0 }
class: dbend
# Law of Large Numbers

Let $X_1, X_2, ‚Ä¶, X_n$ are independent and identically distributed (iid) random
variables with finite mean $ùîº\big[X_i\big] = Œº < ‚àû$, and let
$$XÃÑ_n = \frac{1}{n}\sum_{i=1}^N X_i.$$

~~~
## Weak Law of Large Numbers

The average $XÃÑ_N$ converges in probability to $Œº$:
$$XÃÑ_n \stackrel{p}{\rightarrow} Œº \textrm{~~when~~} n ‚Üí ‚àû\textrm{,~~i.e.,~~}\lim_{n ‚Üí ‚àû} P\big(|XÃÑ_n - Œº|<Œµ\big) = 1.$$

~~~
## Strong Law of Large Numbers

The average $XÃÑ_N$ converges to $Œº$ almost surely:
$$XÃÑ_n \stackrel{a.s.}{\longrightarrow} Œº \textrm{~~when~~} n ‚Üí ‚àû\textrm{,~~i.e.,~~}P\Big(\lim_{n ‚Üí ‚àû} XÃÑ_n = Œº\Big) = 1.$$

---
# Exploitation versus Exploration

Choosing a greedy action is **exploitation** of current estimates. We however also
need to **explore** the space of actions to improve our estimates.

~~~
To make sure our estimates converge to the true values, we need to sample every
action unlimited number of times.

~~~
An _$Œµ$-greedy_ method follows the greedy action with probability $1-Œµ$, and
chooses a uniformly random action with probability $Œµ$.

---
# $Œµ$-greedy Method

![w=87%,mw=60%,h=center,f=right](e_greedy.svgz)

Considering the 10-armed bandit problem:

- we generate 2000 random instances

  - each $q_*(a)$ is sampled from $ùìù(0, 1)$

~~~
- for every instance, we run 1000 steps of the $Œµ$-greedy method
  - we consider $Œµ$ of 0, 0.01, 0.1

~~~
- we plot the averaged results over the 2000 instances

---
section: MDP
class: section
# Markov Decision Process

---
# Markov Decision Process

![w=85%,h=center,v=middle](mdp.svgz)

~~~~
# Markov Decision Process

![w=47%,h=center](mdp.svgz)

A **Markov decision process** (MDP) is a quadruple $(ùì¢, ùìê, p, Œ≥)$,
where:
- $ùì¢$ is a set of states,
~~~
- $ùìê$ is a set of actions,
~~~
- $p(S_{t+1} = s', R_{t+1} = r | S_t = s, A_t = a)$ is a probability that
  action $a ‚àà ùìê$ will lead from state $s ‚àà ùì¢$ to $s' ‚àà ùì¢$, producing a **reward** $r ‚àà ‚Ñù$,
~~~
- $Œ≥ ‚àà [0, 1]$ is a **discount factor** (we always use $Œ≥=1$ and finite episodes in this course).

~~~
Let a **return** $G_t$ be $G_t ‚âù ‚àë_{k=0}^‚àû Œ≥^k R_{t + 1 + k}$. The goal is to optimize $ùîº[G_0]$.

---
# Episodic and Continuing Tasks

If the agent-environment interaction naturally breaks into independent
subsequences, usually called **episodes**, we talk about **episodic tasks**.
Each episode then ends in a special **terminal state**, followed by a reset
to a starting state (either always the same, or sampled from a distribution
of starting states).

~~~
In episodic tasks, it is often the case that every episode ends in at most
$H$ steps. These **finite-horizon tasks** then can use discount factor $Œ≥=1$,
because the return $G ‚âù ‚àë_{t=0}^H Œ≥^t R_{t + 1}$ is well defined.

~~~
If the agent-environment interaction goes on and on without a limit, we instead
talk about **continuing tasks**. In this case, the discount factor $Œ≥$ needs
to be sharply smaller than 1.


---
# Policy

A **policy** $œÄ$ computes a distribution of actions in a given state, i.e.,
$œÄ(a | s)$ corresponds to a probability of performing an action $a$ in state
$s$.

~~~
We will model a policy using a neural network with parameters $‚ÜíŒ∏$:
$$œÄ(a | s; ‚ÜíŒ∏).$$

~~~
If the number of actions is finite, we consider the policy to be a categorical
distribution and utilize the $\softmax$ output activation as in supervised
classification.

---
# (State-)Value and Action-Value Functions

To evaluate a quality of a policy, we define **value function** $v_œÄ(s)$, or
**state-value function**, as
$$\begin{aligned}
  v_œÄ(s) & ‚âù ùîº_œÄ\left[G_t \middle| S_t = s\right] = ùîº_œÄ\left[‚àë\nolimits_{k=0}^‚àû Œ≥^k R_{t+k+1} \middle| S_t=s\right] \\
         & = ùîº_{A_t ‚àº œÄ(s)} ùîº_{S_{t+1},R_{t+1} ‚àº p(s,A_t)} \big[R_{t+1} + Œ≥v_œÄ(S_{t+1})\big] \\
         & = ùîº_{A_t ‚àº œÄ(s)} ùîº_{S_{t+1},R_{t+1} ‚àº p(s,A_t)} \big[R_{t+1}
           + Œ≥ ùîº_{A_{t+1} ‚àº œÄ(S_{t+1})} ùîº_{S_{t+2},R_{t+2} ‚àº p(S_{t+1},A_{t+1})} \big[R_{t+2} + ‚Ä¶ \big]\big]
\end{aligned}$$

~~~
An **action-value function** for a policy $œÄ$ is defined analogously as
$$q_œÄ(s, a) ‚âù ùîº_œÄ\left[G_t \middle| S_t = s, A_t = a\right] = ùîº_œÄ\left[‚àë\nolimits_{k=0}^‚àû Œ≥^k R_{t+k+1} \middle| S_t=s, A_t = a\right].$$

~~~
The value function and the state-value function can be easily expressed using one another:
$$\begin{aligned}
  v_œÄ(s) &= ùîº_{a‚àºœÄ}\big[q_œÄ(s, a)\big], \\
  q_œÄ(s, a) &= ùîº_{s', r ‚àº p}\big[r + Œ≥v_œÄ(s')\big]. \\
\end{aligned}$$

---
# Optimal Value Functions

**Optimal state-value function** is defined as
$$v_*(s) ‚âù \max_œÄ v_œÄ(s),$$
~~~
and **optimal action-value function** is defined analogously as
$$q_*(s, a) ‚âù \max_œÄ q_œÄ(s, a).$$

~~~
Any policy $œÄ_*$ with $v_{œÄ_*} = v_*$ is called an **optimal policy**. Such policy
can be defined as $œÄ_*(s) ‚âù \argmax_a q_*(s, a) = \argmax_a ùîº[R_{t+1} + Œ≥v_*(S_{t+1}) | S_t = s, A_t = a]$.
When multiple actions maximize $q_*(s, a)$, the optimal policy can
stochastically choose any of them.

~~~
## Existence
In finite-horizon tasks or if $Œ≥ < 1$, there always exists a unique optimal
state-value function, a¬†unique optimal action-value function, and a (not necessarily
unique) optimal policy.

---
section: REINFORCE
class: section
# The REINFORCE Algorithm

---
# Policy Gradient Methods

We train the policy
$$œÄ(a | s; ‚ÜíŒ∏)$$
by maximizing the expected return $v_œÄ(s)$.

~~~
To that account, we need to compute its **gradient** $‚àá_{‚ÜíŒ∏} v_œÄ(s)$.

---
# Policy Gradient Theorem

Assume that $ùì¢$ and $ùìê$ are finite, $Œ≥=1$, and that maximum episode length $H$ is also finite.

Let $œÄ(a | s; ‚ÜíŒ∏)$ be a parametrized policy. We denote the initial state
distribution as $h(s)$ and the on-policy distribution under $œÄ$ as $Œº(s)$.
Let also $J(‚ÜíŒ∏) ‚âù ùîº_{s‚àºh} v_œÄ(s)$.

~~~
Then
$$‚àá_{‚ÜíŒ∏} v_œÄ(s) ‚àù ‚àë_{s'‚ààùì¢} P(s ‚Üí ‚Ä¶ ‚Üí s'|œÄ) ‚àë_{a ‚àà ùìê} q_œÄ(s', a) ‚àá_{‚ÜíŒ∏} œÄ(a | s'; ‚ÜíŒ∏)$$
~~~
and
$$‚àá_{‚ÜíŒ∏} J(‚ÜíŒ∏) ‚àù ‚àë_{s‚ààùì¢} Œº(s) ‚àë_{a ‚àà ùìê} q_œÄ(s, a) ‚àá_{‚ÜíŒ∏} œÄ(a | s; ‚ÜíŒ∏),$$

~~~
where $P(s ‚Üí ‚Ä¶ ‚Üí s'|œÄ)$ is the probability of getting to state $s'$ when starting
from state $s$, after any number of 0, 1, ‚Ä¶ steps.


---
# Proof of Policy Gradient Theorem

$\displaystyle ‚àáv_œÄ(s) = ‚àá \Big[ ‚àë\nolimits_a œÄ(a|s; ‚ÜíŒ∏) q_œÄ(s, a) \Big]$

~~~
$\displaystyle \phantom{‚àáv_œÄ(s)} = ‚àë\nolimits_a \Big[ q_œÄ(s, a) ‚àá œÄ(a|s; ‚ÜíŒ∏) + œÄ(a|s; ‚ÜíŒ∏) ‚àá q_œÄ(s, a) \Big]$

~~~
$\displaystyle \phantom{‚àáv_œÄ(s)} = ‚àë\nolimits_a \Big[ q_œÄ(s, a) ‚àá œÄ(a|s; ‚ÜíŒ∏) + œÄ(a|s; ‚ÜíŒ∏) ‚àá \big(‚àë\nolimits_{s', r} p(s', r|s, a)(r + v_œÄ(s'))\big) \Big]$

~~~
$\displaystyle \phantom{‚àáv_œÄ(s)} = ‚àë\nolimits_a \Big[ q_œÄ(s, a) ‚àá œÄ(a|s; ‚ÜíŒ∏) + œÄ(a|s; ‚ÜíŒ∏) \big(‚àë\nolimits_{s'} p(s'|s, a) ‚àá v_œÄ(s')\big) \Big]$

~~~
_We now expand $v_œÄ(s')$._

~~~
$\displaystyle \phantom{‚àáv_œÄ(s)} = ‚àë\nolimits_a \Big[ q_œÄ(s, a) ‚àá œÄ(a|s; ‚ÜíŒ∏) + œÄ(a|s; ‚ÜíŒ∏) \Big(‚àë\nolimits_{s'} p(s'|s, a)\Big(\\
                \quad\qquad\qquad ‚àë\nolimits_{a'} \Big[ q_œÄ(s', a') ‚àá œÄ(a'|s'; ‚ÜíŒ∏) + œÄ(a'|s'; ‚ÜíŒ∏) \big(‚àë\nolimits_{s''} p(s''|s', a') ‚àá v_œÄ(s'')\big) \Big] \Big) \Big) \Big]$

~~~
_Continuing to expand all $v_œÄ(s'')$, we obtain the following:_

$\displaystyle ‚àáv_œÄ(s) = ‚àë\nolimits_{s'‚ààùì¢} ‚àë\nolimits_{k=0}^H P(s ‚Üí s'\textrm{~in~}k\textrm{~steps~}|œÄ) ‚àë\nolimits_{a ‚àà ùìê} q_œÄ(s', a) ‚àá_{‚ÜíŒ∏} œÄ(a | s'; ‚ÜíŒ∏).$

---
# Proof of Policy Gradient Theorem

To finish the proof of the first part, it is enough to realize that
$$‚àë\nolimits_{k=0}^H P(s ‚Üí s'\textrm{~in~}k\textrm{~steps~}|œÄ) ‚àù P(s ‚Üí ‚Ä¶ ‚Üí s'|œÄ).$$

~~~
For the second part, we know that
$$‚àá_{‚ÜíŒ∏} J(‚ÜíŒ∏) = ùîº_{s ‚àº h} ‚àá_{‚ÜíŒ∏} v_œÄ(s) ‚àù ùîº_{s ‚àº h} ‚àë_{s'‚ààùì¢} P(s ‚Üí ‚Ä¶ ‚Üí s'|œÄ) ‚àë_{a ‚àà ùìê} q_œÄ(s', a) ‚àá_{‚ÜíŒ∏} œÄ(a | s'; ‚ÜíŒ∏),$$
~~~
therefore using the fact that $Œº(s') = ùîº_{s ‚àº h} P(s ‚Üí ‚Ä¶ ‚Üí s'|œÄ)$ we get
$$‚àá_{‚ÜíŒ∏} J(‚ÜíŒ∏) ‚àù ‚àë_{s‚ààùì¢} Œº(s) ‚àë_{a ‚àà ùìê} q_œÄ(s, a) ‚àá_{‚ÜíŒ∏} œÄ(a | s; ‚ÜíŒ∏).$$

~~~
Finally, note that the theorem can be proven with infinite $ùì¢$ and $ùìê$; and
also for infinite episodes when discount factor $Œ≥<1$.

---
# REINFORCE Algorithm

The REINFORCE algorithm (Williams, 1992) directly uses the policy gradient
theorem, minimizing $-J(‚ÜíŒ∏) ‚âù -ùîº_{s‚àºh} v_œÄ(s)$. The loss gradient is then
$$‚àá_{‚ÜíŒ∏} -J(‚ÜíŒ∏) ‚àù -‚àë_{s‚ààùì¢} Œº(s) ‚àë_{a ‚àà ùìê} q_œÄ(s, a) ‚àá_{‚ÜíŒ∏} œÄ(a | s; ‚ÜíŒ∏) = -ùîº_{s ‚àº Œº} ‚àë_{a ‚àà ùìê} q_œÄ(s, a) ‚àá_{‚ÜíŒ∏} œÄ(a | s; ‚ÜíŒ∏).$$

~~~
However, the sum over all actions is problematic. Instead, we rewrite it to an
expectation which we can estimate by sampling:
$$‚àá_{‚ÜíŒ∏} -J(‚ÜíŒ∏) ‚àù ùîº_{s ‚àº Œº} ùîº_{a ‚àº œÄ} q_œÄ(s, a) ‚àá_{‚ÜíŒ∏} -\log œÄ(a | s; ‚ÜíŒ∏),$$
~~~
where we used the fact that
$$‚àá_{‚ÜíŒ∏} \log œÄ(a | s; ‚ÜíŒ∏) = \frac{1}{œÄ(a | s; ‚ÜíŒ∏)} ‚àá_{‚ÜíŒ∏} œÄ(a | s; ‚ÜíŒ∏).$$

---
# REINFORCE Algorithm

REINFORCE therefore minimizes the loss $-J(‚ÜíŒ∏)$ with gradient
$$ùîº_{s ‚àº Œº} ùîº_{a ‚àº œÄ} q_œÄ(s, a) ‚àá_{‚ÜíŒ∏} -\log œÄ(a | s; ‚ÜíŒ∏),$$
where we estimate the $q_œÄ(s, a)$ by a single sample.

~~~
Note that the loss is just a weighted variant of negative log-likelihood (NLL),
where the sampled actions play a role of gold labels and are weighted according
to their return.

~~~
![w=75%,h=center](reinforce.svgz)

---
# REINFORCE Algorithm Example Performance

![w=30%,v=middle](stochastic_policy_example.svgz)![w=69%,v=middle](reinforce_performance.svgz)

---
section: Baseline
class: section
# REINFORCE with Baseline

---
# REINFORCE with Baseline

The returns can be arbitrary: better-than-average and worse-than-average
returns cannot be recognized from the absolute value of the return.

~~~
Hopefully, we can generalize the policy gradient theorem using a baseline $b(s)$
to
$$‚àá_{‚ÜíŒ∏} J(‚ÜíŒ∏) ‚àù ‚àë_{s‚ààùì¢} Œº(s) ‚àë_{a ‚àà ùìê} \big(q_œÄ(s, a) - \boldsymbol{b(s)}\big) ‚àá_{‚ÜíŒ∏} œÄ(a | s; ‚ÜíŒ∏).$$

~~~
The baseline $b(s)$ can be a function or even a random variable, as long as it
does not depend on $a$, because
$$‚àë_a b(s) ‚àá_{‚ÜíŒ∏} œÄ(a | s; ‚ÜíŒ∏) = b(s) ‚àë_a ‚àá_{‚ÜíŒ∏} œÄ(a | s; ‚ÜíŒ∏) = b(s) ‚àá_{‚ÜíŒ∏} ‚àë_a œÄ(a | s; ‚ÜíŒ∏) = b(s) ‚àá_{‚ÜíŒ∏} 1 = 0.$$

---
# REINFORCE with Baseline

A good choice for $b(s)$ is $v_œÄ(s)$, which can be shown to minimize the
variance of the gradient estimator. Such baseline reminds centering of the
returns, given that
$$v_œÄ(s) = ùîº_{a ‚àº œÄ} q_œÄ(s, a).$$

~~~
Then, better-than-average returns are positive and worse-than-average returns
are negative.

~~~
Of course, we need a way to estimate the $v_œÄ(s)$ baseline. The usual approach
is to approximate it by another neural network model. That model is trained
using mean square error of the predicted and observed returns.

---
# REINFORCE with Baseline

In REINFORCE with baseline, we train:
1. the _policy network_ using the REINFORCE algorithm, and
~~~
2. the _value network_ by minimizing the mean squared error.

![w=85%,h=center](reinforce_with_baseline.svgz)

---
# REINFORCE with Baseline Example Performance


![w=40%,h=center,mh=48%](stochastic_policy_example.svgz)

![w=48%](reinforce_performance.svgz)![w=52%](reinforce_with_baseline_comparison.svgz)

---
section: NAS
class: section
# Neural Architecture Search

---
# Neural Architecture Search: NASNet, 2017

- We can design neural network architectures using reinforcement learning.

~~~
- The designed network is encoded as a sequence of elements, and is generated
  using an **RNN controller**, which is trained using the REINFORCE with baseline
  algorithm.

![w=55%,h=center](nasnet_overview.svgz)

~~~
- For every generated sequence, the corresponding network is trained on CIFAR-10
  and the development accuracy is used as a return.

---
# Neural Architecture Search: NASNet, 2017

The overall architecture of the designed network is fixed and only the Normal
Cells and Reduction Cells are generated by the controller.

![w=29%,h=center](nasnet_overall.svgz)

---
# Neural Architecture Search: NASNet, 2017

- Each cell is composed of $B$ blocks ($B=5$ is used in NASNet).
~~~
- Each block is designed by a RNN controller generating 5 parameters.

![w=80%,h=center](nasnet_rnn_controller.svgz)

![w=60%,mw=50%,h=center](nasnet_block_steps.svgz)![w=80%,mw=50%,h=center](nasnet_operations.svgz)

- Every block is designed by a RNN controller generating individual operations.

---
# Neural Architecture Search: NASNet, 2017

The final Normal Cell and Reduction Cell chosen from 20k architectures
(500GPUs, 4days).

![w=77%,h=center](nasnet_blocks.svgz)

---
# EfficientNet Search

EfficientNet changes the search in three ways.

~~~
- Computational requirements are part of the return. Notably, the goal is to
  find an architecture $m$ maximizing
  $$\operatorname{DevelopmentAccuracy}(m) ‚ãÖ \left(\frac{\textrm{TargetFLOPS=400M}}{\operatorname{FLOPS}(m)}\right)^{0.07},$$
~~~
  where the constant $0.07$ balances the accuracy and FLOPS (_the constant comes
  from an empirical observation that doubling the FLOPS brings about 5% relative
  accuracy gain, and $1.05 = 2^Œ≤$_ gives $Œ≤ ‚âà 0.0704$).

~~~
- It uses a different search space allowing to control kernel sizes and
  channels in different parts of the architecture (compared to using the same
  cell everywhere as in NASNet).

~~~
- Training directly on ImageNet, but only for 5 epochs.

~~~
In total, 8k model architectures are sampled, and PPO algorithm is used
instead of the REINFORCE with baseline.

---
# EfficientNet Search

![w=100%](mnasnet_overall.svgz)

![w=30%,f=right](mnasnet_parameters.svgz)

The overall architecture consists of 7 blocks, each described by 6 parameters
‚Äì 42 parameters in total, compared to 50 parameters of the NASNet search space.

---
# EfficientNet-B0 Baseline Network

![w=100%](../05/efficientnet_architecture.svgz)
---
section: RLWhatNext
# What Next

If you find deep reinforcement learning interesting, I have a whole course
dedicated to it: **NPFL139 ‚Äì Deep Reinforcement Learning**.

~~~
- It covers a range of reinforcement learning algorithms, from the basic
  ones to more advanced algorithms utilizing deep neural networks.

~~~
- Summer semester, 3/2 C+Ex, 8 e-credits, similar structure as Deep learning.

~~~
- An elective (povinnƒõ voliteln√Ω) course in the programs:
  - Artificial Intelligence,
  - Language Technologies and Computational Linguistics.

---
section: GenerativeModels
class: section
# Generative Models

---
# Generative Models

![w=76%,h=center](stable_diffusion.jpg)

---
# Generative Models

![w=50%](hands_v4.jpg)
~~~
![w=47%](hands_v5.jpg)

---
# Generative Models

Generative models are given a set of realizations of a random variable $‚Åá‚Üíx$ and
their goal is to estimate $P(‚Üíx)$.

~~~
Usually the goal is to be able to sample from $P(‚Åá‚Üíx)$, but sometimes an
explicit calculation of $P(‚Üíx)$ is also possible.

---
# Deep Generative Models

![w=25%,h=center](generative_model.svgz)

One possible approach to estimate $P(‚Üíx)$ is to assume that the random variable
$‚Åá‚Üíx$ depends on a **latent variable** $‚Åá‚Üíz$:
$$P(‚Üíx) = ‚àë_{‚Üíz} P(‚Üíz) P(‚Üíx | ‚Üíz) = ùîº_{‚Üíz ‚àº P(‚Åá‚Üíz)} P(‚Üíx | ‚Üíz).$$

~~~
We use neural networks to estimate the conditional probability
$P_{‚ÜíŒ∏}(‚Üíx | ‚Üíz)$.

---
class: section
# AutoEncoders

---
# AutoEncoders

![w=50%,h=center](ae.svgz)

- Autoencoders are useful for unsupervised feature extraction, especially when
  performing input compression (i.e., when the dimensionality of the latent
  space $‚Üíz$ is smaller than the dimensionality of the input).

~~~
- When $‚Üíx + ‚ÜíŒµ$ is used as input, autoencoders can perform denoising.

~~~
- However, the latent space $‚Üíz$ does not need to be fully covered, so
  a randomly chosen $‚Üíz$ does not need to produce a valid $‚Üíx$.

---
# AutoEncoders

![w=100%,v=middle](ae_latent_space.png)

---
section: VAE
class: section
# Variational AutoEncoders

---
# Variational AutoEncoders

We assume $P(‚Åá‚Üíz)$ is fixed and independent on $‚Åá‚Üíx$.

~~~
We approximate $P(‚Üíx | ‚Üíz)$ using a neural network $P_{‚ÜíŒ∏}(‚Üíx | ‚Üíz)$, the
**decoder**.

~~~
However, in order to train an autoencoder, we need to know the
‚Äúinverse‚Äù $P_{‚ÜíŒ∏}(‚Üíz | ‚Üíx)$, which cannot be usually computed directly.

~~~
Therefore, we approximate $P_{‚ÜíŒ∏}(‚Üíz | ‚Üíx)$ by another trainable neural network
$Q_{‚ÜíœÜ}(‚Üíz | ‚Üíx)$, the **encoder**.

---
style: .katex-display { margin: .65em 0 }
# Jensen's Inequality

To derive a loss for training variational autoencoders, we first formulate
the Jensen's inequality.

~~~
![w=95%,mw=37%,h=right,f=right](../02/convex_2d.svgz)

Recall that convex functions by definition fulfil that for $‚Üíu, ‚Üív$ and real $0
‚â§ t ‚â§ 1$,
$$f(t‚Üíu + (1-t)‚Üív) ‚â§ tf(‚Üíu) + (1-t)f(‚Üív).$$

~~~
The **Jensen's inequality** generalizes the above property to any _convex_
combination of points: if we have $‚Üíu_i ‚àà ‚Ñù^D$ and weights $w_i ‚àà ‚Ñù^+$ such
that $‚àë_i w_i = 1$, it holds that

![w=95%,mw=37%,h=right,f=right](jensens_inequality.png)

$$f\big(‚àë_i w_i ‚Üíu_i\big) ‚â§ ‚àë_i w_i f\big(‚Üíu_i\big).$$

~~~
The Jensen's inequality can be formulated also for probability distributions
(whose expectation can be considered an infinite convex combination):

$$f\big(ùîº[‚Åá‚Üíu]\big) ‚â§ ùîº_{‚Åá‚Üíu} \big[f(‚Åá‚Üíu)\big].$$

---
# VAE ‚Äì Loss Function Derivation

Our goal will be to maximize the log-likelihood as usual, but we need to express
it using the latent variable $‚Üíz$:
$$\log P_{‚ÜíŒ∏}(‚Üíx) = \log ùîº_{P(‚Üíz)} \big[P_{‚ÜíŒ∏}(‚Üíx | ‚Üíz)\big].$$

~~~
However, approximating the expectation using a single sample has monstrous
variance, because for most $‚Üíz$, $P_{‚ÜíŒ∏}(‚Üíx | ‚Üíz)$ will be nearly zero.

~~~
We therefore turn to our _encoder_, which is able for a given $‚Üíx$ to generate
‚Äúits‚Äù $‚Üíz$:

~~~
$\displaystyle \kern6em\mathllap{\log P_{‚ÜíŒ∏}(‚Üíx)} = \log ùîº_{P(‚Üíz)} \big[P_{‚ÜíŒ∏}(‚Üíx | ‚Üíz)\big]$

$\displaystyle \kern6em{} = \log ùîº_{Q_{‚ÜíœÜ}(‚Üíz|‚Üíx)} \bigg[P_{‚ÜíŒ∏}(‚Üíx | ‚Üíz) ‚ãÖ \frac{P(‚Üíz)}{Q_{‚ÜíœÜ}(‚Üíz|‚Üíx)}\bigg]\kern2.75em\textcolor{gray}{\textrm{\footnotesize now we use the Jensen's inequality}}$

~~~
$\displaystyle \kern6em{} ‚â• ùîº_{Q_{‚ÜíœÜ}(‚Üíz|‚Üíx)} \bigg[\log P_{‚ÜíŒ∏}(‚Üíx | ‚Üíz) + \log\frac{P(‚Üíz)}{Q_{‚ÜíœÜ}(‚Üíz|‚Üíx)}\bigg]$

~~~
$\displaystyle \kern6em{} = ùîº_{Q_{‚ÜíœÜ}(‚Üíz|‚Üíx)} \big[\log P_{‚ÜíŒ∏}(‚Üíx | ‚Üíz)\big] - D_\textrm{KL}\big(Q_{‚ÜíœÜ}(‚Üíz|‚Üíx) \| P(‚Üíz)\big) ‚âù ùìõ(‚ÜíŒ∏, ‚ÜíœÜ;‚Åá‚Üíx).$

---
# VAE ‚Äì Variational (or Evidence) Lower Bound

The resulting **variational lower bound** or **evidence lower bound** (ELBO),
denoted $ùìõ(‚ÜíŒ∏, ‚ÜíœÜ;‚Åá‚Üíx)$, can be also defined explicitly as:
$$ùìõ(‚ÜíŒ∏, ‚ÜíœÜ;‚Åá‚Üíx) = \log P_{‚ÜíŒ∏}(‚Üíx) - D_\textrm{KL}\big(Q_{‚ÜíœÜ}(‚Üíz | ‚Üíx) \| P_{‚ÜíŒ∏}(‚Üíz | ‚Üíx)\big).$$

~~~
Because KL-divergence is nonnegative, $ùìõ(‚ÜíŒ∏, ‚ÜíœÜ;‚Åá‚Üíx) ‚â§ \log P_{‚ÜíŒ∏}(‚Üíx).$

~~~
By using simple properties of conditional and joint probability, we get that

~~~
$\displaystyle \kern9em\mathllap{ùìõ(‚ÜíŒ∏, ‚ÜíœÜ;‚Åá‚Üíx)} = ùîº_{Q_{‚ÜíœÜ}(‚Üíz | ‚Üíx)} \big[\log P_{‚ÜíŒ∏}(‚Üíx) + \log P_{‚ÜíŒ∏}(‚Üíz | ‚Üíx) - \log Q_{‚ÜíœÜ}(‚Üíz | ‚Üíx)\big]$

~~~
$\displaystyle \kern9em{} = ùîº_{Q_{‚ÜíœÜ}(‚Üíz | ‚Üíx)} \big[\log P_{‚ÜíŒ∏}(‚Üíx, ‚Üíz) - \log Q_{‚ÜíœÜ}(‚Üíz | ‚Üíx)\big]$

~~~
$\displaystyle \kern9em{} = ùîº_{Q_{‚ÜíœÜ}(‚Üíz | ‚Üíx)} \big[\log P_{‚ÜíŒ∏}(‚Üíx | ‚Üíz) + \log P(‚Üíz) - \log Q_{‚ÜíœÜ}(‚Üíz | ‚Üíx)\big]$

~~~
$\displaystyle \kern9em{} = ùîº_{Q_{‚ÜíœÜ}(‚Üíz | ‚Üíx)} \big[\log P_{‚ÜíŒ∏}(‚Üíx | ‚Üíz)\big] - D_\textrm{KL}\big(Q_{‚ÜíœÜ}(‚Üíz | ‚Üíx) \| P(‚Üíz)\big).$

---
# Variational AutoEncoders Training

$$-ùìõ(‚ÜíŒ∏, ‚ÜíœÜ;‚Åá‚Üíx) = ùîº_{Q_{‚ÜíœÜ}(‚Üíz | ‚Üíx)} \big[-\log P_{‚ÜíŒ∏}(‚Üíx | ‚Üíz)\big] + D_\textrm{KL}\big(Q_{‚ÜíœÜ}(‚Üíz | ‚Üíx) \| P(‚Üíz)\big)$$

- We train a VAE by minimizing the $-ùìõ(‚ÜíŒ∏, ‚ÜíœÜ;‚Åá‚Üíx)$.

~~~
- The $ùîº_{Q_{‚ÜíœÜ}(‚Üíz | ‚Üíx)}$ is estimated using a single sample.
~~~
- The distribution $Q_{‚ÜíœÜ}(‚Üíz | ‚Üíx)$ is parametrized as a normal distribution
  $ùìù(‚Üíz | ‚ÜíŒº, ‚ÜíœÉ^2)$, with the model predicting $‚ÜíŒº$ and $‚ÜíœÉ$ given $‚Üíx$.
~~~
  - In order for $‚ÜíœÉ$ to be positive, we can use $\exp$ activation function
    (so that the network predicts $\log ‚ÜíœÉ$ before the activation), or for
    example a $\operatorname{softplus}$ activation function.
~~~
  - The normal distribution is used, because we can sample from it efficiently,
    we can backpropagate through it and we can compute $D_\textrm{KL}$
    analytically; furthermore, if we decide to parametrize $Q_{‚ÜíœÜ}(‚Üíz | ‚Üíx)$ using
    mean and variance, the maximum entropy principle suggests we should use the
    normal distribution.
~~~
- We use a prior $P(‚Üíz) = ùìù(‚Üí0, ‚ÜíI)$.

---
# Variational AutoEncoders Training

$$-ùìõ(‚ÜíŒ∏, ‚ÜíœÜ;‚Åá‚Üíx) = ùîº_{Q_{‚ÜíœÜ}(‚Üíz | ‚Üíx)} \big[-\log P_{‚ÜíŒ∏}(‚Üíx | ‚Üíz)\big] + D_\textrm{KL}\big(Q_{‚ÜíœÜ}(‚Üíz | ‚Üíx) \| P(‚Üíz)\big)$$

![w=50%,h=center](vae_architecture.svgz)

Note that the loss has 2 intuitive components:
- **reconstruction loss** ‚Äì starting with $‚Üíx$, passing though $Q_{‚ÜíœÜ}$, sampling
  $‚Üíz$ and then passing through $P_{‚ÜíŒ∏}$ should arrive back at $‚Üíx$;
~~~
- **latent loss** ‚Äì over all $‚Üíx$, the distribution of $Q_{‚ÜíœÜ}(‚Üíz | ‚Üíx)$ should be as close as
  possible to the prior $P(‚Üíz) = ùìù(‚Üí0, ‚ÜíI)$, which is independent on $‚Üíx$.

---
# Variational AutoEncoders ‚Äì Reparametrization Trick

In order to backpropagate through $‚Üíz‚àºQ_{‚ÜíœÜ}(‚Üíz | ‚Üíx)$, note that if
$$‚Üíz ‚àº ùìù(‚ÜíŒº, ‚ÜíœÉ^2),$$

~~~
we can write $‚Üíz$ as
$$‚Üíz ‚àº ‚ÜíŒº + ‚ÜíœÉ ‚äô ùìù(‚Üí0, ‚ÜíI).$$

~~~
Such formulation then allows differentiating $‚Üíz$ with respect to
$‚ÜíŒº$ and $‚ÜíœÉ$ and is called a **reparametrization trick** (Kingma and Welling, 2013).

---
# Variational AutoEncoders ‚Äì Reparametrization Trick

![w=100%,v=middle](reparametrization_trick.png)

---
# Variational AutoEncoders ‚Äì Reparametrization Trick

![w=100%,h=center](vae_architecture_reparametrized.svgz)

---
# Variational AutoEncoders

![w=80%,h=center](vae_manifold.svgz)

---
# Variational AutoEncoders

![w=100%,v=middle](vae_dimensionality.svgz)

---
# Variational AutoEncoders

![w=100%,v=middle](latent_space.png)

---
# Variational AutoEncoders ‚Äì Too High Latent Loss

![w=50%,h=center](vae_high_latent_loss.png)

---
# Variational AutoEncoders ‚Äì Too High Reconstruction Loss

![w=50%,h=center](vae_high_reconstruction_loss.png)

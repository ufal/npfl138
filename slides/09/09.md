title: NPFL138, Lecture 9
class: title, langtech, cc-by-sa

# Seq2seq, NMT, Transformer

## Milan Straka

### April 15, 2025

---
section: Seq2seq
class: section
# Sequence-to-Sequence Architecture (Seq2seq)

---
# Sequence-to-Sequence Architecture

Sequence-to-Sequence is a name for an architecture allowing to produce an
arbitrary output sequence $y_1, …, y_M$ from an input sequence
$→x_1, …, →x_N$.

~~~
Unlike span labeling/CTC, no assumptions are necessary and we condition each output
sequence element on all input sequence elements and all already generated output
sequence elements:
$$P\big(→y \mid ⇉X\big) = ∏_{i=1}^M P\big(y_i \mid →x_1, …, →x_N, y_1, …, y_{i-1}\big).$$

---
# Sequence-to-Sequence Architecture

![w=100%,h=center,v=middle](seq2seq_architecture.svgz)

---
# Sequence-to-Sequence Architecture from Original Papers

![w=80%,h=center](seq2seq.svgz)
~~~
![w=30%,h=center](encoder_decoder.svgz)

---
# Sequence-to-Sequence Architecture

## Training

![w=50%,f=right](../07/sequence_prediction_training.svgz)

The so-called **teacher forcing** is used during training – the gold outputs are
used as inputs during training.

~~~
## Inference

![w=50%,f=right](../07/sequence_prediction_inference.svgz)

During inference, the network processes its own predictions – such an approach
is called **autoregressive decoding**.

Usually, the generated logits are processed by an $\argmax$, the chosen word
embedded and used as next input.

---
section: Tying
class: section
# Word Embedding Tying

---
# Tying Word Embeddings

![w=86%,mw=30%,h=center,f=right](tying_embeddings.svgz)

In the decoder, we both:
- embed the previous prediction, using a matrix of size $ℝ^{V × D}$, where
  $V$ is the vocabulary size and $D$ is the embedding size;
- classify the hidden state into current prediction, using a matrix of
  size $ℝ^{D × V}$.

~~~
Both these matrices have similar meaning – they represent words in the embedding
space (the first explicitly represents words by the embeddings, the second
produces logits by computing weighted cosine similarity of the inputs and
columns of the weight matrix).

~~~
Therefore, it makes sense to **tie** these matrices, i.e., to represent one of
them as a transposition of the other.
~~~
- However, while the embedding matrix should usually have constant variance per
  dimension, the output layer should keep the variance of the RNN output;
  therefore, the output layer matrix is usually the embedding matrix divided by $\sqrt D$.

---
section: Attention
class: section
# Attention

---
# Attention

![w=100%,v=middle](seq2seq_attention.svgz)

---
class: section
# Bahdanau (or Additive) Attention

---
# Bahdanau Attention

![w=35%,f=right](attention.svgz)

As another input during decoding, we add _context vector_ $c_i$:
$$→s_i = f(→s_{i-1}, →y_{i-1}, →c_i).$$

~~~
We compute the context vector as a weighted combination of source sentence
encoded outputs:
$$→c_i = ∑_j α_{ij} →h_j$$

~~~
The weights $α_{ij}$ are softmax of $e_{ij}$ over $j$,
$$→α_i = \softmax(→e_i),$$
with $e_{ij}$ being
$$e_{ij} = →v^\top \tanh(⇉V→h_j + ⇉W→s_{i-1} + →b).$$

---
# Bahdanau Attention Implementation

![w=43%,h=center](attention_implementation.svgz)

---
# Trained Attention Visualization

![w=45%,h=center](attention_visualization.svgz)

---
class: section
# Luong (or Dot-Product) Attention

---
# Luong Attention

In the described _Bahdanau_ (_additive_) attention, we performed
$$e_{ij} = →v^\top \tanh(⇉V→h_j + ⇉W→s_{i-1} + →b).$$

~~~
There are however other methods how $⇉V→h_j$ and $⇉W→s_{i-1}$ can be combined,
most notably the _Luong_ (or _dot-product_) attention, which uses just a dot
product:
$$e_{ij} = \big(⇉V→h_j\big)^T\big(⇉W→s_{i-1}\big).$$

~~~
The latter is easier to implement, but may sometimes be more difficult to train
(scaling helps a bit, wait for the Transformer self-attention description); both
approaches are used in quite a few papers.

---
section: SubWords
class: section
# Subword Units (BPE, WordPieces)

---
# Subword Units – BPE

Translate **subword units** instead of words. The subword units can be generated
in several ways, the most commonly used are:

~~~
- **BPE**:
  Using the _byte pair encoding_ algorithm. Start with individual characters plus
  a special end-of-word symbol $•$. Then, merge the most occurring symbol pair
  $A, B$ by a new symbol $AB$, with the symbol pair never crossing word boundary
  (so that the end-of-word symbol cannot be inside a subword).

~~~
  Considering text with words _low, lowest, newer, wider_, a possible sequence
  of merges:

  $$\begin{aligned}
    r \,\,\, • & → r• \\
    l \,\,\, o & → lo \\
    lo \,\,\, w & → low \\
    e \,\,\, r• & → er• \\
  \end{aligned}$$

~~~
  The BPE algorithm is executed on the training data, and it generates the
  resulting dictionary, merging rules, and training data encoded using
  this dictionary.

---
# Subword Units – BPE

- The end-of-word symbol was described to be at the end of a subword in the
  original paper.
~~~
- However, in existing implementations, we usually represent
  **beginning-of-word** symbol at the beginning of a subwords
  instead (usually a suitable encoded space).
~~~
- Furthermore, when considering multilingual models, there are in fact quite
  a lot of characters. Therefore, some tokenizers allow representing bytes of UTF-8
  encoding.
  - Byte-level BPE or BBPE allows even subword splitting at arbitrary
    boundaries (inside a single UTF-8-encoded codepoint).
~~~
  - Sentence-piece model splits at Unicode codepoint boundaries, but can represent
    unknown characters using bytes of UTF-8 encoding.
~~~
- For example, a phrase “Přelétavý motýlek” can be tokenized as

```python
# Using sentence-piece implementation of BPE:
['▁Pře', 'lé', 'ta', 'vý', '▁mot', 'ý', 'lek']
# Byte-level BPE, first trained on English data, the second on Czech data:
['P', 'Å', 'Ļ', 'el', 'Ã©t', 'av', 'Ã', '½', 'Ġmot', 'Ã', '½', 'le', 'k']
['PÅĻe', 'lÃ©t', 'avÃ½', 'ĠmotÃ½', 'lek']
```

---
# Subword Units – WordPieces

- **WordPieces**:
  Given a text divided into subwords, we can compute unigram probability of
  every subword, and then get the likelihood of the text under a unigram language
  model by multiplying the probabilities of the subwords in the text.

~~~
  When we have only a text and a subword dictionary, we divide the text in
  a greedy fashion, iteratively choosing the longest existing subword.

~~~
  When constructing the subwords, we again start with individual characters
  (compared to BPE, we have a _start-of-word_ character instead of an
  _end-of-word_ character), and then repeatedly join such a pair of subwords
  that increases the unigram language model likelihood the most.

~~~
  - In the original implementation, the input data were once in a while
    “reparsed” (retokenized) in a greedy fashion with the up-to-date dictionary.
    However, the recent implementations do not seem to do it – but they
    retokenize the training data with the final dictionary, contrary to the BPE
    approach.

~~~
For both approaches, usually quite little subword units are used (32k-64k),
often generated on the union of the two vocabularies of the source and target
languages (the so-called _joint BPE_ or _shared wordpieces_).

---
# BPE and WordPieces Comparison

Both the BPE and the WordPieces give very similar results; the biggest
difference is that during the inference:
- for BPE, the sequence of merges must be performed in the same order as during
  the construction of the BPE (because we use the output of BPE as training
  data),
~~~
- for WordPieces, it is enough to find longest matches from the subword
  dictionary (because we reprocessed the training data with the final dictionary);
~~~
- note that the above difference is mostly artificial – if we reparsed the
  training data in the BPE approach, we could also perform “greedy
  tokenization”.

~~~
Of course, the two algorithms also differ in the way how they choose the pair of
subwords to merge.

~~~
Both algorithms are implemented in quite a few libraries, most notably the
`sentencepiece` library and the Hugging Face `tokenizers` package.

---
section: GNMT
class: section
# Google Neural Machine Translation (GNMT)

---
# Google NMT

![w=95%,h=center](gnmt_overview.png)

---
# Google NMT

![w=80%,h=center](gnmt_rating.png)

---
# Beyond one Language Pair

![w=75%,h=center](image_labeling.svgz)

---
# Beyond one Language Pair

![w=70%,h=center](vqa.svgz)

---
# Multilingual and Unsupervised Translation

Many attempts at multilingual translation.

- Individual encoders and decoders, shared attention.

- Shared encoders and decoders.

~~~
Surprisingly, even unsupervised translation can be performed.
By unsupervised we understand settings where we have access to large
monolingual corpora, but no parallel data.

~~~
In 2019, the best unsupervised systems were on par with the best 2014 supervised
systems.

![w=73%,h=center](umt_results.svgz)

~~~
Nowadays, language models like ChatGPT can be also considered unsupervised
machine translation, and then achieve superior performance without explicit
parallel data.

---
section: Transformer
class: section
# The Transformer Architecture

---
# Attention is All You Need

For some sequence processing tasks, _sequential_ processing (as performed by
recurrent neural networks) of its elements might be too restrictive.

~~~
Instead, we may want to be able to combine sequence elements independently on
their distance.

~~~
Such processing is allowed in the **Transformer** architecture, originally
proposed for neural machine translation in 2017 in _Attention is All You Need_
paper.

---
# Transformer

![w=33%,h=center](transformer.png)

---
# Transformer

![w=77%,h=center](illustrated_transformer_encoder_decoder.png)

---
# Transformer

![w=100%,v=middle](illustrated_transformer_layer.png)

---
# Transformer

![w=82%,h=center](illustrated_transformer_encoder.png)

---
section: SelfAttention
class: section
# Transformer – Self-Attention

---
# Transformer – Self-Attention

Assume that we have a sequence of $n$ words represented using a matrix $⇉X ∈ ℝ^{n×d}$.

~~~
The attention module for queries $⇉Q ∈ ℝ^{n×d_k}$, keys $⇉K ∈ ℝ^{n×d_k}$ and values $⇉V ∈ ℝ^{n×d_v}$ is defined as:
$$\textrm{Attention}(⇉Q, ⇉K, ⇉V) = \softmax\left(\frac{⇉Q ⇉K^\top}{\sqrt{d_k}}\right)⇉V.$$

~~~
The queries, keys and values are computed from the input word representations $⇉X$
using a linear transformation as
$$\begin{aligned}
  ⇉Q &= ⇉X ⇉W^Q \\
  ⇉K &= ⇉X ⇉W^K \\
  ⇉V &= ⇉X ⇉W^V \\
\end{aligned}$$
for trainable weight matrices $⇉W^Q, ⇉W^K ∈ ℝ^{d×d_k}$ and $⇉W^V ∈ ℝ^{d×d_v}$.

---
# Transformer – Self-Attention

![w=80%,h=center](illustrated_transformer_self_attention_inputs.png)

---
# Transformer – Self-Attention

![w=53%,h=center](illustrated_transformer_self_attention_outputs.png)

---
# Transformer – Self-Attention

![w=100%,v=middle](illustrated_transformer_self_attention_example.png)

---
# Transformer – Self-Attention

![w=40%](illustrated_transformer_self_attention_calculation_1.png)![w=60%](illustrated_transformer_self_attention_calculation_2.png)

---
# Transformer – Multihead Attention

Multihead attention is used in practice. Instead of using one huge attention, we
split queries, keys and values to several groups (similar to how ResNeXt works),
compute the attention in each of the groups separately, concatenate the
results and multiply them by a matrix $⇉W^O$.

![w=75%,h=center](transformer_multihead.png)

---
# Transformer – Multihead Attention

![w=85%,h=center](illustrated_transformer_attention_heads_input.png)

---
# Transformer – Multihead Attention

![w=42%,v=middle](illustrated_transformer_attention_heads_output.png)![w=58%](illustrated_transformer_attention_heads_output_2.png)

---
# Transformer – Multihead Attention

![w=90%,h=center](illustrated_transformer_multihead_attention.png)

---
# Transformer – Multihead Attention

![w=37%,f=right](multihead_attention_splitting.png)

When multihead attention is used, we first generate query/key/value vectors of
the same dimension, and then split them into smaller pieces. Therefore,
multihead attention does not increase complexity (much) and is analogous to
ResNeXt/GroupNorm.

![w=58%,h=center](multihead_attention_overview.png)

---
# Why Attention

![w=100%,v=middle](transformer_attentions.svgz)

---
class: section
# Transformer – Feed Forward Networks

---
# Transformer – Feed Forward Networks

## Feed Forward Networks

The self-attention is complemented with FFN layers, which is a fully connected
ReLU layer with four times as many hidden units as inputs, followed by another
fully connected layer without activation.

![w=90%,h=center](../07/layer_norm_residual.svgz)

---
# Transformer – Post-LN Configuration including Residuals

![w=53%,h=center](illustrated_transformer_encoder_residual.png)

---
# Transformer – Pre-LN Configuration

![w=100%](transformer_preln.svgz)

---
# Transformer – Decoder

![w=72%,v=middle](illustrated_transformer_decoder.png)![w=28%](transformer.png)

---
# Transformer – Decoder

![w=28%,f=right](transformer.png)

## Masked Self-Attention

During decoding, the self-attention must attend only to earlier positions in the
output sequence.

~~~
This is achieved by **masking** future positions, i.e., zeroing their weights out,
which is usually implemented by setting them to $-∞$ before the $\softmax$ calculation.

~~~

## Encoder-Decoder Attention

In the encoder-decoder attentions, the _queries_ comes from the decoder, while the
_keys_ and the _values_ originate from the encoder.

---
section: PosEmbed
class: section
# Transformer – Positional Embeddings

---
# Transformer – Positional Embeddings

![w=91%,h=center](illustrated_transformer_positional_encoding.png)

---
# Transformer – Positional Embeddings

## Positional Embeddings

We need to encode positional information (which was implicit in RNNs).

~~~
- Learned embeddings for every position.

~~~
- Sinusoids of different frequencies:
  $$\small\begin{aligned}
    \textrm{PE}_{(\textit{pos}, 2i)} & = \sin\left(\textit{pos} / 10000^{2i/d}\right) \\
    \textrm{PE}_{(\textit{pos}, 2i + 1)} & = \cos\left(\textit{pos} / 10000^{2i/d}\right)
  \end{aligned}$$

~~~
  This choice of functions should allow the model to attend to relative
  positions, since for any fixed $k$, $\textrm{PE}_{\textit{pos} + k}$ is
  a linear function of $\textrm{PE}_\textit{pos}$, because
  $$\small\begin{aligned}
    \textrm{PE}_{(\textit{pos}+k, 2i)}
      &= \sin\left((\textit{pos}+k) / 10000^{2i/d}\right) \\
      &= \sin\left(\textit{pos} / 10000^{2i/d}\right) ⋅ \cos\left(k / 10000^{2i/d}\right) + \cos\left(\textit{pos} / 10000^{2i/d}\right) ⋅ \sin\left(k / 10000^{2i/d}\right) \\
      &= \textit{offset}_{(k,2i)} ⋅ \textrm{PE}_{(\textit{pos}, 2i)} + \textit{offset}_{(k, 2i+1)} ⋅ \textrm{PE}_{(\textit{pos}, 2i + 1)}.
  \end{aligned}$$

---
# Transformer – Positional Embeddings

## Positional Embeddings

### Sinusoids of different frequencies

In the original description of positional embeddings (the one used on the
previous slide), the sines and cosines are interleaved, so for $d=6$, the
positional embeddings would look like:

$$\small\textrm{PE}_\textit{pos} = \Big(\textstyle
  \sin\big(\frac{\textit{pos}}{10000^0}\big), \cos\big(\frac{\textit{pos}}{10000^0}\big),
  \sin\big(\frac{\textit{pos}}{10000^{1/3}}\big), \cos\big(\frac{\textit{pos}}{10000^{1/3}}\big),
  \sin\big(\frac{\textit{pos}}{10000^{2/3}}\big), \cos\big(\frac{\textit{pos}}{10000^{2/3}}\big)
\Big).$$

~~~
However, in practice, most implementations concatenate first all the sines and
only then all the cosines:

$$\small\widehat{\textrm{PE}}_\textit{pos} = \Big(\textstyle
  \sin\big(\frac{\textit{pos}}{10000^0}\big), \sin\big(\frac{\textit{pos}}{10000^{1/3}}\big), \sin\big(\frac{\textit{pos}}{10000^{2/3}}\big),
  \cos\big(\frac{\textit{pos}}{10000^0}\big), \cos\big(\frac{\textit{pos}}{10000^{1/3}}\big), \cos\big(\frac{\textit{pos}}{10000^{2/3}}\big)
\Big).$$

This is also how we visualize the positional embeddings on the following slides.

---
# Transformer – Positional Embeddings

![w=65%,h=center](transformer_positional_embeddings_16.svgz)

---
# Transformer – Positional Embeddings

![w=65%,h=center](transformer_positional_embeddings_64.svgz)

---
# Transformer – Positional Embeddings

![w=65%,h=center](transformer_positional_embeddings_512.svgz)

---
section: Training
class: section
# Transformer – Training

---
# Transformer – Training

## Regularization

The network is regularized by:
- dropout of input embeddings,
~~~
- dropout of each sub-layer, just before it is added to the residual
  connection (and then normalized),
~~~
- label smoothing.

~~~
Default dropout rate and also label smoothing weight is 0.1.

~~~
## Parallel Execution
Because of the _masked attention_, training can be performed in parallel.

~~~
However, inference is still sequential.

---
# Transformer – Training

## Optimizer

Adam optimizer (with $β_2=0.98$, smaller than the default value of $0.999$)
is used during training, with the learning rate decreasing proportionally to
inverse square root of the step number.

~~~
## Warmup
Furthermore, during the first
$\textit{warmup\_steps}$ updates, the learning rate is increased linearly from
zero to its target value.

$$\textit{learning\_rate} = \frac{1}{\sqrt{d_\textrm{model}}} \min\left(\frac{1}{\sqrt{\textit{step\_num}}}, \frac{\textit{step\_num}}{\textit{warmup\_steps}} ⋅ \frac{1}{\sqrt{\textit{warmup\_steps}}}\right).$$

~~~
In the original paper, 4000 warmup steps were proposed.

~~~
Note that the goal of warmup is mostly to prevent divergence early in training;
the Pre-LN configuration usually trains well even without warmup.

---
# Transformers Results

![w=100%](transformer_results.svgz)

Subwords were constructed using BPE with a shared vocabulary of about 37k tokens.

---
# Transformers Ablations on En→De newtest2014 Dev

![w=72%,h=center](transformer_ablations.svgz)

The PPL is _perplexity per wordpiece_, where perplexity is $e^{H(P)}$,
i.e., $e^\textit{loss}$ in our case.

---
# Decoder-Only Models

In seq2seq architecture, we have both an **encoder** and a **decoder**.

~~~
However, for text generation (chatbots, for example), a **decoder-only** model suffices.

~~~
- Examples include GPT-1 (2018), GPT-2 (2019), ChatGPT, Copilot, Llama, …

~~~
- The decoder-only models are trained as language models, i.e., they estimate
  conditional probability of a word given its previous context (like Elmo).

~~~
- They consist purely of the decoder part of a Transformer (so they do not
  contain neither an encoder nor encoder-decoder attention; consequently, all
  their self-attentions are masked).

~~~
- On https://bbycroft.net/llm, you can find a 3D visualization and the
  description of the Transformer computation steps of several GPT models.

---
class: summary
# Summary

- **Seq2seq** is a general architecture of producing an output sequence from an
  input sequence.

- It is usually trained using **teacher forcing**, and use **autoregressive decoding**.

- **Attention** allows focusing on any part of a sequence in every time step.

- **Transformer** provides more powerful sequence-to-sequence architecture and also
  sequence element representation architecture compared to **RNNs**, but requires
  **substantially more** data.
  - When data are plentiful, best models for processing text, speech, and vision
    data utilize the Transformer architecture (together with convolutions in the
    vision domain).

- In seq2seq architecture, we have both an **encoder** and the **decoder**.
  However, text generation (i.e., in chatbots) is usually performed by
  **decoder-only** models.

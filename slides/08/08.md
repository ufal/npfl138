title: NPFL138, Lecture 8
class: title, langtech, cc-by-sa
style: .algorithm { background-color: #eee; padding: .5em }

# Structured Prediction, CTC, Word2Vec

## Milan Straka

### April 8, 2025

---
section: Span Labeling
class: section
# Structured Prediction

---
# Structured Prediction

Consider generating a sequence of $y_1, \ldots, y_N ‚àà ùì®^N$ given input
$‚Üíx_1, \ldots, ‚Üíx_N$.

~~~
Predicting each sequence element independently models the distribution $P(y_i | ‚áâX)$.

![w=40%,h=center](labeling_independent.svgz)

~~~
However, there may be dependencies among the $y_i$ themselves, in the sense
that not all sequences of $y_i$ are valid; but when generating each $y_i$
independently, the model might be capable of generating also invalid
sequences.

---
class: section
# Span Labeling

---
# Structured Prediction ‚Äì Span Labeling

Consider for example **named entity recognition**, whose goal is to locate
_named entities_, which are single words or sequences of multiple words
denoting real-world objects, concepts, and events.
~~~
The most common types of named entities include:
- `PER`: _people_, including names of individuals, historical figures, and even
  fictional characters;
~~~
- `ORG`: _organizations_, incorporating companies, government agencies,
  educational institutions, and others;
~~~
- `LOC`: _locations_, encompassing countries, cities, geographical features,
  addresses.

~~~
Compared to part-of-speech tagging, locating named entities is much more
challenging ‚Äì named entity mentions are generally multi-word spans, and
arbitrary number of named entities can appear in a sentence (consequently,
we cannot use accuracy for evaluation; F1-score is commonly used).

~~~
Named entity recognition is an instance of a **span labeling** task, where
the goal is to locate and classify **spans**, i.e., continuous subsequences
of the original sequence.

---
# Span Labeling ‚Äì BIO Encoding

A possible approach to a span labeling task is to classify every sequence
element using a specialized tag set. A common approach is to use the
**BIO** encoding, which consists of
~~~
- `O`: _outside_, the given element is not part of any span;

~~~
- `B-PER`, `B-ORG`, `B-LOC`, ‚Ä¶: _beginning_, the element is first in a new span;
~~~
- `I-PER`, `I-ORG`, `I-LOC`, ‚Ä¶: _inside_, a continuation element of an existing
  span.

~~~
In a **valid** sequence, the `I-TYPE` **must follow** either `B-TYPE` or `I-TYPE`.

~~~
(Formally, the described scheme is IOB-2 format; there exists quite a few other
possibilities like IOB-1, IEO, BILOU, ‚Ä¶)

~~~
The described encoding can represent any set of continuous typed spans (when no spans
overlap, i.e., a single element can belong to at most one span).

---
# Span Labeling ‚Äì BIO Encoding

However, when predicting each of the element tags independently, invalid
sequences might be created.

~~~
- We might decide to ignore it and use heuristics capable of recovering the spans
  from invalid sequences of BIO tags.

~~~
- We might employ a decoding algorithm producing the most probable **valid
  sequence** of tags during prediction.
~~~
  - However, during training we do not consider the BIO tags validity.

~~~
- We might use a different loss enabling the model to consider only
  valid BIO tag sequences also during training.

---
# Span Labeling ‚Äì Decoding Algorithm

Let $‚Üíx_1, \ldots, ‚Üíx_N$ be an input sequence.

Our goal is to produce an output sequence $y_1, ‚Ä¶, y_N$, where each $y_t ‚àà ùì®$
with $Y$ classes.

~~~
Assume we have a model predicting $p(y_t = k | ‚áâX; ‚ÜíŒ∏)$, a probability that the
$t$-th output element $y_t$ is the class $k$.

~~~
However, only some sequences $‚Üíy$ are valid.
~~~
We now make an assumption that the validity of a sequence depends only on the
validity of **neighboring** output classes. In other words, if all neighboring
pairs of output elements are valid, the whole sequence is.

~~~
- The validity of neighboring pairs can be described by a transition matrix
  $‚áâA ‚àà \{0, 1\}^{Y√óY}$.
~~~
- Such an approach allows expressing the (in)validity of a BIO tag sequence.
~~~
  - However, the current formulation does not enforce conditions on the first
    and the last tag.

~~~
    If needed (for example to disallow `I-TYPE` as the first tag), we can
    add fixed $y_0$ and/or $y_{N+1}$ imposing conditions on $y_1$ and/or
    $y_N$, respectively.

---
style: .katex-display { margin: .6em 0 }
# Span Labeling ‚Äì Decoding Algorithm

Let us denote $Œ±_t(k)$ the log probability of the most probable output sequence
of $t$ elements with the last one being $k$.
~~~
- We use log probability to avoid rounding to zero (for 32bit floats, $10^{-46}‚âà0$).

~~~
We can compute $Œ±_t(k)$ efficiently using dynamic programming. The core idea is
the following:

![w=35%,h=center](crf_composability.svgz)

~~~
$$Œ±_t(k) = \log p(y_t=k | ‚áâX; ‚ÜíŒ∏) + \max\nolimits_{j,\textrm{~such~that~}A_{j,k}\textrm{~is~valid}} Œ±_{t-1}(j).$$

~~~
If we consider $\log A_{j,k}$ to be $-‚àû$ when $A_{j,k}=0$, we can rewrite the above as
$$Œ±_t(k) = \log p(y_t=k | ‚áâX; ‚ÜíŒ∏) + \max\nolimits_j \big(Œ±_{t-1}(j) + \log A_{j,k}\big).$$

~~~
The resulting algorithm is also called the **Viterbi algorithm**, and it is also
a search for the path of maximum length in an acyclic graph.

---
# Span Labeling ‚Äì Decoding Algorithm

<div class="algorithm">

**Inputs**: Input sequence of length $N$, tag set with $Y$ tags.  
**Inputs**: Model computing $p(y_t = k | ‚áâX; ‚ÜíŒ∏)$, a probability that $y_t$
should have the class $k$.
**Inputs**: Transition matrix $‚áâA ‚àà ‚Ñù^{Y√óY}$ indicating _valid_ and _invalid_
transitions.  
**Outputs**: The most probable sequence $‚Üíy$ consisting of valid transitions
only.  
**Time Complexity**: $ùìû(N ‚ãÖ Y^2)$ in the worst case.

~~~
- For $t = 1, \ldots, N$:
  - For $k = 1, \ldots, Y:$
    - $Œ±_t(k) ‚Üê \log p(y_t=k | ‚áâX; ‚ÜíŒ∏)$¬†¬†_logits (unnormalized log probs) can also be used_
    - If $t > 1$:
      - $Œ≤_t(k) ‚Üê \argmax\nolimits_{j,\textrm{~such~that~}A_{j,k}\textrm{~is~valid}} Œ±_{t-1}(j)$
      - $Œ±_t(k) ‚Üê Œ±_t(k) + Œ±_{t-1}\big(Œ≤_t(k)\big)$
~~~
- The most probable sequence has logprob $\max Œ±_N$, last element $y_N ‚Üê \argmax Œ±_N$, and the other elements can be recovered
  by traversing $Œ≤$ from $t=N$ down to $t=2$.
</div>

---
# Span Labeling ‚Äì Other Approaches

With deep learning models, constrained decoding is usually sufficient to deliver
high performance even without considering labeling validity during training.

~~~
However, there also exist approaches considering label dependence during training:

~~~
- **Maximum Entropy Markov Models**

  We might model the dependencies by explicitly conditioning on the previous
  label:
  $$P(y_i | ‚áâX, y_{i-1}).$$

~~~
  Then, each label is predicted by a softmax from a hidden state and a
  _previous label_.
  ![w=35%,h=center](labeling_memm.svgz)

~~~
  The decoding can still be performed by a dynamic programming algorithm.

---
# Span Labeling ‚Äì Other Approaches

- **Conditional Random Fields (CRF)**

  In the simplest variant, Linear-chain CRF, usually abbreviated only to CRF,
  can be considered an extension of softmax ‚Äì instead of a sequence of
  independent softmaxes, it is a sentence-level softmax, with additional weights
  for neighboring sequence elements.

~~~
  We start by defining a score of a label sequence $‚Üíy$ as
  $$s(‚áâX, ‚Üíy; ‚ÜíŒ∏, ‚áâA) = f(y_1 | ‚áâX; ‚ÜíŒ∏) + ‚àë\nolimits_{i=2}^N \big(‚áâA_{y_{i-1}, y_i} + f(y_i | ‚áâX; ‚ÜíŒ∏)\big),$$
~~~
  and define the probability of a label sequence $‚Üíy$ using $\softmax$:
  $$p(‚Üíy | ‚áâX) = \softmax_{‚Üíz ‚àà Y^N}\big(s(‚áâX, ‚Üíz)\big)_{‚Üíy}.$$

~~~
  The probability $\log p(‚Üíy_\textrm{gold} | ‚áâX)$ can be efficiently computed
  using dynamic programming in a differentiable way, so it can be used in NLL
  computation.

~~~
  For more details, see [Lecture 8 of NPFL114 2022/23 slides](https://ufal.mff.cuni.cz/~straka/courses/npfl114/2223/slides/?08).

---
section: CTC
class: section
# Connectionist Temporal Classification (CTC)

---
# Connectionist Temporal Classification

Let us again consider generating a sequence of $y_1, \ldots, y_M$ given input
$‚Üíx_1, \ldots, ‚Üíx_N$, but this time $M ‚â§ N$, and there is no explicit alignment
of $‚Üíx$ and $y$ in the gold data.

~~~
![w=100%,mh=90%,v=middle](ctc_example.svgz)

---
# Connectionist Temporal Classification

![w=60%,h=center](ctc_naive_alignment.svgz)

Naive alignment has two problems:
~~~
- repeated symbols cannot be represented;
~~~
- not every input element might be classifiable as the target label.

---
# Connectionist Temporal Classification

We enlarge the set of the output labels by a ‚Äì (**blank**), and perform a classification for every
input element to produce an **extended labeling** (in contrast to the original **regular labeling**).
We then post-process it by the following rules (denoted as $ùìë$):
1. We collapse multiple neighboring occurrences of the same symbol into one.
2. We remove the blank ‚Äì.

~~~
<br>
![w=95%,mw=49%](ctc_alignment_steps.svgz)
~~~
![w=95%,mw=49%,h=right](ctc_valid_invalid_alignments.svgz)

---
# Connectionist Temporal Classification

![w=95%,h=right,mw=56%,f=right](ctc_full_collapse_from_audio.svgz)

Because the explicit alignment of inputs and labels is not known, we consider
_all possible_ alignments.

~~~
Denoting the probability of label $l$ at time $t$ as $p_l^t$, we define
$$Œ±^t(s) ‚âù ‚àë_{\substack{\textrm{extended}\\\textrm{labelings~}‚ÜíœÄ:\\ùìë(‚ÜíœÄ_{1:t}) = ‚Üíy_{1:s}}} ‚àè_{i=1}^t p_{œÄ_i}^i.$$

---
# Connectionist Temporal Classification

## Computation

When aligning an extended labeling to a regular one, we need to consider
whether the extended labeling ends by a _blank_ or not. We therefore define
$$\begin{aligned}
  Œ±_-^t(s) &‚âù ‚àë_{\substack{\textrm{extended}\\\textrm{labelings~}‚ÜíœÄ:\\ùìë(‚ÜíœÄ_{1:t}) = ‚Üíy_{1:s}, œÄ_t=-}} ‚àè_{i=1}^t p_{œÄ_i}^i \\
  Œ±_*^t(s) &‚âù ‚àë_{\substack{\textrm{extended}\\\textrm{labelings~}‚ÜíœÄ:\\ùìë(‚ÜíœÄ_{1:t}) = ‚Üíy_{1:s}, œÄ_t‚â†-}} ‚àè_{i=1}^t p_{œÄ_i}^i

\end{aligned}$$
and compute $Œ±^t(s)$ as $Œ±_-^t(s) + Œ±_*^t(s)$.

---
style: h2 { margin-top: 0pt }
# Connectionist Temporal Classification

## Computation ‚Äì Initialization

![w=35%,f=right](ctc_computation.svgz)

We initialize $Œ±^1$ as follows:
- $Œ±_-^1(0) ‚Üê p_-^1$
- $Œ±_*^1(1) ‚Üê p_{y_1}^1$
- all other $Œ±^1$ to zeros

~~~
## Computation ‚Äì Induction Step

We then proceed recurrently according to:
- $Œ±_-^t(s) ‚Üê p_-^t \big(Œ±_*^{t-1}(s) + Œ±_-^{t-1}(s)\big)$

~~~
- $Œ±_*^t(s) ‚Üê \begin{cases}
  p_{y_s}^t\big(Œ±_*^{t-1}(s) + Œ±_-^{t-1}(s-1) + Œ±_*^{t-1}(s-1)\big)\textrm{, if }y_s‚â†y_{s-1}\\
  p_{y_s}^t\big(Œ±_*^{t-1}(s) + Œ±_-^{t-1}(s-1) + \sout{Œ±_*^{t-1}(s-1)}\big)\textrm{, if }y_s=y_{s-1}\\
\end{cases}$

~~~
  We can write the update as $p_{y_s}^t\big(Œ±_*^{t-1}(s) + Œ±_-^{t-1}(s-1) + [y_s‚â†y_{s-1}] ‚ãÖ Œ±_*^{t-1}(s-1)\big)$.

~~~
You can visit https://distill.pub/2017/ctc/ for additional nice and detailed description.

---
class: dbend
style: .katex-display { margin: .7em 0 }
# Computation ‚Äì Log Probabilities

Analogously to the Viterbi algorithm, in practice we need to compute with log
probabilities, not probabilities as described on the previous slide.

~~~
However, we also need to add log probabilities, i.e., for given $a$ and $b$,
compute
$$\log(e^a + e^b).$$

~~~
Such operation is available as `torch.logaddexp`, `keras.ops.logaddexp`,
`np.logaddexp`.

~~~
- Note that straightforward implementation of $\operatorname{logaddexp}$ can
  easily overflow, because $e^{90} ‚âà ‚àû$ for 32bit floats.
~~~
  However, assuming $a ‚â• b$, we can rearrange it to

  $$\operatorname{logaddexp}(a, b) ‚âù \log(e^a + e^b) = \textcolor{blue}{\log}\big(\textcolor{blue}{e^a}(1 + e^b/e^a)\big)
  = \textcolor{blue}{a} + \log(1 + \underbrace{e^{b-a}}_{‚â§ 1}).$$

~~~
- Additionally, if we have a whole sequence of log probabilities $‚Üía$, we can sum
  all of them using `torch.logsumexp` or `keras.ops.logsumexp`.

$$\operatorname{logsumexp}(‚Üía) ‚âù \log({\textstyle\small ‚àë_{a_i} e^{a_i}})
= \textcolor{blue}{\log}\big(\textcolor{blue}{e^{\max ‚Üía}}({\textstyle\small ‚àë_{a_i} e^{a_i}/e^{\max ‚Üía}})\big)
= \textcolor{blue}{\max ‚Üía} + \log({\textstyle\small ‚àë_{a_i} e^{a_i-\max ‚Üía}}).$$

---
section: CTCDecoding
# CTC Decoding

Unlike BIO-tag structured prediction, nobody knows how to perform CTC decoding
optimally in polynomial time.

~~~
The key observation is that while an optimal extended labeling can be extended
into an optimal labeling of a greater length, the same does not apply to
a regular labeling. The problem is that regular labeling corresponds to many
extended labelings, which are modified each in a different way during an
extension of the regular labeling.

~~~
![w=72%,h=center](ctc_decoding.svgz)

---
# CTC Decoding

## Beam Search

![w=80%,h=center](ctc_beam_search.svgz)

---
# CTC Decoding

## Beam Search

![w=96%,h=center](ctc_prefix_beam_search.svgz)

---
# CTC Decoding

## Beam Search

To perform a beam search, we keep $k$ best **regular** (non-extended) labelings.
Specifically, for each regular labeling $‚Üíy$ we keep both $Œ±^t_-(‚Üíy)$ and
$Œ±^t_*(‚Üíy)$, which are probabilities of all (modulo beam search) extended
labelings of length $t$ which produce the regular labeling $‚Üíy$; we therefore
keep $k$ regular labelings with the highest $Œ±^t_-(‚Üíy) + Œ±^t_*(‚Üíy)$.

~~~
To compute the best regular labelings for a longer prefix of extended labelings,
for each regular labeling in the beam we consider the following cases:
~~~
- adding a _blank_ symbol, i.e., contributing to $Œ±^{t+1}_-(‚Üíy)$ both from
  $Œ±^t_-(‚Üíy)$ and $Œ±^t_*(‚Üíy)$;
~~~
- adding a non-blank symbol, i.e., contributing to $Œ±^{t+1}_*(‚Ä¢)$ from
  $Œ±^t_-(‚Üíy)$ and contributing to a possibly different $Œ±^{t+1}_*(‚Ä¢)$ from
  $Œ±^t_*(‚Üíy)$.

~~~
Finally, we merge the resulting candidates according to their regular labeling, and
keep only the $k$ best.

---
section: Word2Vec
class: section
# Word2Vec: Unsupervised Word Embeddings

---
# Unsupervised Word Embeddings

The embeddings can be trained for each task separately.

~~~

However, a method of precomputing word embeddings have been proposed, based on
_distributional hypothesis_:

> **Words that are used in the same contexts tend to have similar meanings**.

~~~
The distributional hypothesis is usually attributed to Firth (1957):
> _You shall know a word by a company it keeps._

---
# Word2Vec

![w=70%,h=center](word2vec.svgz)

Mikolov et al. (2013) proposed two very simple architectures for precomputing
word embeddings, together with a C multi-threaded implementation `word2vec`.

---
# Word2Vec

Vector arithmetics seem to capture lexical semantics.

![w=80%,h=center,mh=80%,v=bottom](w2v_relations.svgz)

---
# Word2Vec

![w=100%](word2vec_composability.svgz)

---
# Word2Vec ‚Äì SkipGram Model

![w=50%,h=center,mh=64%](word2vec.svgz)

Considering input word $w_i$ and output $w_o$, the Skip-gram model defines
$$p(w_o | w_i) ‚âù \frac{e^{‚áâV_{w_i}^\top ‚áâW_{w_o}}}{‚àë_w e^{‚áâV_{w_i}^\top ‚áâW_w}}.$$
After training, the final embeddings are the rows of the $‚áâV$ matrix.

---
# Word2Vec ‚Äì Hierarchical Softmax

Instead of a large softmax, we construct a binary tree over the words, with
a sigmoid classifier for each node.

If word $w$ corresponds to a path $n_1, n_2, \ldots, n_L$, we define
$$p_\textrm{HS}(w | w_i) ‚âù ‚àè_{j=1}^{L-1} œÉ(\textrm{[+1 if }n_{j+1}\textrm{  is right child else -1]} ‚ãÖ ‚áâV_{w_i}^\top ‚áâW_{n_j}).$$

---
# Word2Vec ‚Äì Negative Sampling

Instead of a large softmax, we could train individual sigmoids for all words.

~~~
We could also only sample several _negative examples_. This gives rise to the
following _negative sampling_ objective (instead of just summing all the
sigmoidal losses):
$$l_\textrm{NEG}(w_o, w_i) ‚âù -\log œÉ(‚áâV_{w_i}^\top ‚áâW_{w_o}) - ‚àë_{j=1}^k ùîº_{w_j ‚àº P(w)} \log \big(1 - œÉ(‚áâV_{w_i}^\top ‚áâW_{w_j})\big).$$

~~~
The usual value of negative samples $k$ is 5, but it can be even 2 for extremely
large corpora.

~~~
Each expectation in the loss is estimated using a single sample.

~~~
For $P(w)$, both uniform and unigram distribution $U(w)$ work, but
$$U(w)^{3/4}$$
outperforms them significantly (this fact has been reported in several papers by
different authors).

---
section: CLEs
# Recurrent Character-level WEs

![w=80%,h=center](../07/cle_rnn_examples.svgz)

---
# Convolutional Character-level WEs

![w=100%](../07/cle_cnn_examples.svgz)

---
section: Subword Embeddings
class: section
# Subword Embeddings

---
section: Subword Embeddings
# Character N-grams

Another simple idea appeared simultaneously in three nearly simultaneous
publications as [Charagram](https://arxiv.org/abs/1607.02789), [Subword Information](https://arxiv.org/abs/1607.04606) or [SubGram](http://link.springer.com/chapter/10.1007/978-3-319-45510-5_21).

A word embedding is a sum of the word embedding plus embeddings of its character
_n_-grams. Such embedding can be pretrained using same algorithms as `word2vec`.

~~~
The implementation can be
- dictionary based: only some number of frequent character _n_-grams is kept;
~~~
- hash-based: character _n_-grams are hashed into $K$ buckets
  (usually $K ‚àº 10^6$ is used).

---
# Charagram WEs

![w=100%,v=middle](cle_charagram_examples.svgz)

---
# Charagram WEs

![w=48%,h=center](cle_charagram_ngrams.svgz)

---
# FastText

The word2vec enriched with subword embeddings is implemented in publicly
available `fastText` library https://fasttext.cc/.

~~~
Pre-trained embeddings for 157 languages (including Czech) trained on
Wikipedia and CommonCrawl are also available at
https://fasttext.cc/docs/en/crawl-vectors.html.

---
section: ELMo
class: section
# Embeddings from Language Models (ELMo)

---
# ELMo

At the end of 2017, a new type of _deep contextualized_ word representations was
proposed by Peters et al., called ELMo, **E**mbeddings from **L**anguage
**Mo**dels.

~~~
The ELMo embeddings were based on a two-layer pre-trained LSTM language model,
where a language model predicts following word based on a sentence prefix.
~~~
Specifically, two such models were used, one for the forward direction and the
other one for the backward direction.
~~~

![w=30%](elmo_language_model.png)![w=68%](elmo_bidirectional.png)

---
# ELMo

To compute an embedding of a word in a sentence, the concatenation of the two
language model's hidden states is used.

![w=68%,h=center](elmo_embedding.png)

~~~
To be exact, the authors propose to take a (trainable) weighted combination of
the input embeddings and outputs on the first and second LSTM layers.

---
# ELMo Results

Pre-trained ELMo embeddings substantially improved several NLP tasks.

![w=100%](elmo_results.svgz)

